%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8x]{inputenc}

\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}



\usepackage{enumitem}

\title{Loom Application User Guides}
\date{Apr 07, 2018}
\release{0.7.0}
\author{Veritas Technologies LLC}
\newcommand{\sphinxlogo}{\sphinxincludegraphics{loom.png}\par}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{mcdmp_app_ug::doc}}


\begin{sphinxShadowBox}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id6}}{\hyperref[\detokenize{mcdmp_app_ug:data-analysis-classification-app-user-guide}]{\sphinxcrossref{360 Data Analysis \& Classification App User Guide}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id7}}{\hyperref[\detokenize{mcdmp_app_ug:introduction-to-veritas-360-data-analysis}]{\sphinxcrossref{Introduction to Veritas 360 Data Analysis}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id8}}{\hyperref[\detokenize{mcdmp_app_ug:using-veritas-360-data-analysis}]{\sphinxcrossref{Using Veritas 360 Data Analysis}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id9}}{\hyperref[\detokenize{mcdmp_app_ug:other-data-views}]{\sphinxcrossref{Other data views}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id10}}{\hyperref[\detokenize{mcdmp_app_ug:about-classifying-content-resources}]{\sphinxcrossref{About classifying content resources}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id11}}{\hyperref[\detokenize{mcdmp_app_ug:finding-your-way-around-the-classification-policies-ui}]{\sphinxcrossref{Finding your way around the Classification Policies UI}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id12}}{\hyperref[\detokenize{mcdmp_app_ug:initiating-classification-of-files}]{\sphinxcrossref{Initiating classification of files}}}

\end{itemize}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id13}}{\hyperref[\detokenize{mcdmp_app_ug:veritas-connection-center-user-guide}]{\sphinxcrossref{Veritas Connection Center User Guide}}}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id14}}{\hyperref[\detokenize{mcdmp_app_ug:getting-started-with-veritas-connection-center}]{\sphinxcrossref{Getting started with Veritas Connection Center}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id15}}{\hyperref[\detokenize{mcdmp_app_ug:configuring-on-cloud-content-sources}]{\sphinxcrossref{Configuring On-cloud content sources}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id16}}{\hyperref[\detokenize{mcdmp_app_ug:configuring-on-premises-content-sources}]{\sphinxcrossref{Configuring On-premises content sources}}}

\end{itemize}

\end{itemize}
\end{sphinxShadowBox}

Post Alpha2 release, Loom Platform and Application Ecosystem will comprise of the following applications.  Customers can subscribe to one or more of these applications as per Loom Trial \& Licensing terms and conditions as per \sphinxhref{http://10.67.141.149/shaloo/aggr/pform-idocs/html/lic/lic\_overview.html\#lic-overview}{Loom License and Subscription Model}. Following is a list of Applications slated for Loom Beta:

\# Information Map
\# Information Studio
\# GDPR Readiness
\# SAR Lite

The Loom Applications Team is working on documenting the \DUrole{xref,std,std-ref}{mcdmp\_infomap\_app\_ug}, \DUrole{xref,std,std-ref}{mcdmp\_infostudio\_app\_ug}, \DUrole{xref,std,std-ref}{mcdmp\_gdpr\_app\_ug} and \DUrole{xref,std,std-ref}{mcdmp\_sar\_lite\_app\_ug}.  As of April 5, 2018, these applications are under development. The documentation for the same is work in progress.  For now, we have the following Loom Application User Guides available for reference.

\# 360 Data Analysis
\# Connection Center


\chapter{360 Data Analysis \& Classification App User Guide}
\label{\detokenize{mcdmp_app_ug:mcdmp-vis-app-ug}}\label{\detokenize{mcdmp_app_ug:app-user-guides}}\label{\detokenize{mcdmp_app_ug:data-analysis-classification-app-user-guide}}\label{\detokenize{mcdmp_app_ug:loom-app-user-guides}}
Multi Cloud Data Management Platform provides a highly scalable, multi-tenanted glue which helps several data management applications to collaborate and play well when it comes to enterprise information management.  The Visibility \& Classification app runs on MCDM Platform and  renders your unstructured data in visual context and guides users towards unbiased, information-governance decision-making. Using the dynamic navigation offered by the app, customers can identify areas of risk, areas of value, and areas of waste across their environment and make decisions which reduce information risk and optimize information storage. Besides this, it helps organizations improve unstructured data governance to reduce costs and risk through actionable intelligence into data ownership, usage, and access controls.

You can refer to the \DUrole{xref,std,std-ref}{MCDM Platform App User Guide Outline here}.


\section{Introduction to Veritas 360 Data Analysis}
\label{\detokenize{mcdmp_app_ug:introduction-to-veritas-360-data-analysis}}
This section gives a brief overview of the 360 Data Analysis application and provides insight into the high-level uses cases of the app.


\subsection{What is Veritas 360 Data Analysis}
\label{\detokenize{mcdmp_app_ug:what-is-veritas-360-data-analysis}}
Veritas 360 Data Analysis is a cloud application built on top of the highly available and scalable Multi Cloud Data Management technology platform. The application allows you to visualize the information within your data centers and cloud storage repositories. It also enables you to understand the value and relevance of the information in your data centers and allow you to take action to improve data governance across multiple areas, such as storage, compliance, risk, security and discovery.

The application helps you answers questions such as:
\begin{itemize}
\item {} 
Where does my data live?

\item {} 
What is the value of my information?

\item {} 
Who owns the information and who is using it?

\item {} 
What is the age of my information

\end{itemize}

Veritas 360 Data Analysis provides intelligence about the data that resides on your Network Attached Storage (NAS) devices and cloud storage repositories across geographical locations. It lets you collect information about the data on your various content sources such as Windows file servers, Linux and UNIX file servers, Network Attached Storage (NAS) devices, and cloud accounts to help you visualize the information that is stored within your organization’s data centers. Visibility uses the file and folder metadata cloud and on-premises content sources to help you visualize your data and processes this information and displays various perspectives of the data in your environment in the Visibility console.

Veritas 360 Data Analysis gives you insight into the trends in the growth of the information in your organization and gives you a projection of the storage requirements for the future. The analytics help you plan your capacity requirements. Visibility views give you a  global, regional, data center, and server level overview of storage consumption trends.

It also provides the classification of content such as all files that contain Personally Identifiable Information to help you understand what kind of data is occupying available storage space.

It also detects duplicate data on your file systems. Duplicate detection helps you to ensure that the information is stored appropriately and reclaim storage space on your file servers by applying policies to delete or archive duplicate files and folders.

The dashboard view displays various tiles that enable you to identify the following:
\begin{itemize}
\item {} 
The total file volume for configured locations which helps you quickly review the volume of information that you are managing.

\item {} 
The size and count of the data set that is old or no longer in use.

\item {} 
Non-business information which is being stored on the primary file servers which have no relevance to your organization.

\item {} 
Files that do not have an Active Directory owner.

\item {} 
The top locations, content sources, and repositories such as shares and folders that are the biggest contributors to your data volume.

\item {} 
A summary of the data types that are stored on your file systems which contribute towards the volume of the data. For example, .ppt, .docx, .pdf, etc.

\item {} 
The total size against the age of the data based on the created, modified, or the last accessed date.

\end{itemize}

You can use the intelligence that 360 Data Analysis provides for the following purposes:

\sphinxstylestrong{File system visibility}
\begin{itemize}
\item {} 
You can use the visibility that 360 Data Analysis provides to take decisions such as moving the stale, orphan, or non-business data to cheaper or secondary storage tiers.

\end{itemize}

\sphinxstylestrong{Information analytics}
\begin{itemize}
\item {} 
360 Data Analysis helps you map the location of the information in your organization to help you find the data sets of interest. The analytics that the application provides enables you to identify content sources that may need to be protected or monitored closely.

\item {} 
It also helps you classify items based on their content and metadata.

\end{itemize}

\sphinxstylestrong{Intelligent storage use cases}
\begin{itemize}
\item {} 
Decommission servers and shares - By identifying content sources with little activity on them, you can make decisions to decommission, migrate or re-purpose the storage resources.

\item {} 
Address stale data - By identifying information which is old or no longer in use, you can decide to delete, to archive, or to move to a more appropriate storage tier.

\item {} 
Address non-business data - By identifying information that is of non-typical business use, like MP3s and videos, you can alleviate the disproportional strain on the storage capacity of such file types by archiving or deleting.

\item {} 
Adopt orphan information - By identifying files which lack a current Active Directory owner, you can delete the orphaned information or associate it with an administrator or another custodian.

\item {} 
Smart migrations - By assessing age and activity of information, you can target the migrations of specific or relevant subsets of information to new storage hardware or cloud systems.

\end{itemize}

\sphinxstylestrong{Retention management use cases}
\begin{itemize}
\item {} 
PST identification and retention - By identifying network stored PST files, you can remove them from primary storage. If desired, you can accelerate the migration of the PSTs into Veritas Enterprise Vault using Enterprise Vault’s PST migration toolset.

\end{itemize}

\sphinxstylestrong{Legal, compliance, and security use cases}
\begin{itemize}
\item {} 
Focus eDiscovery collections - Associating potential custodians with their data enables legal teams to focus collection efforts on the specific areas of data owned by the employees involved in a particular matter.

\item {} 
Enhanced protection for priority shares - By identifying content sources with a high level of activity in them, backup teams are better informed when prioritizing areas of their environment to focus backup and recovery efforts.

\item {} 
Identify potential rogue applications - Abnormally high levels of activity in file servers or shares can also be a sign of a rogue application working on the server or share which needs to be shut down.

\end{itemize}


\subsection{Invoking the 360 Data Analysis application}
\label{\detokenize{mcdmp_app_ug:invoking-the-360-data-analysis-application}}
A Loom user with the Tenant admin and the Tenant user role can use the 360 Data Analysis application to gain visibility into the data being monitored by the Loom platform and to manage the data.

Before you can manage data with the 360 Data Analysis app, you must add connections for the content sources that you want monitor. For more information about adding connections, see the Veritas Connection Center App User Guide.

\sphinxstylestrong{To navigate to the 360 Data Analysis application:}
\begin{enumerate}
\item {} 
In the left navigation pane of the Loom UI, click \sphinxstylestrong{Applications}. The \sphinxstylestrong{My Applications} tab displays the applications that you have subscribed to.

\end{enumerate}
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{access_mcdmp_apps}.png}
\end{figure}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Click \sphinxstylestrong{360 Data Analysis}. The application opens.

\end{enumerate}

You can now review the metadata pertaining to the content sources being monitored and submit classification requests for these content sources.

For information about using the application, see {\hyperref[\detokenize{mcdmp_app_ug:using-visibility}]{\sphinxcrossref{\DUrole{std,std-ref}{Using Veritas 360 Data Analysis}}}}.

Refer to \sphinxhref{https://jira.community.veritas.com/browse/IMP-3004}{Jira IMP-3004} ‘


\subsection{What can I do with Veritas 360 Data Analysis?}
\label{\detokenize{mcdmp_app_ug:what-can-i-do-with-veritas-360-data-analysis}}\begin{quote}


\sphinxstrong{See also:}


This is a \sphinxstylestrong{DOC-CONTRIB} Request


\end{quote}

This section contains placeholder for content that requires shared contribution by the Visibility app dev owners.
*  The key use cases for using the 360 Data Analysis app, and the workflows to accomplish specific tasks.

Refer to \sphinxhref{https://jira.community.veritas.com/browse/IMP-3004}{Jira IMP-3004}


\section{Using Veritas 360 Data Analysis}
\label{\detokenize{mcdmp_app_ug:using-visibility}}\label{\detokenize{mcdmp_app_ug:using-veritas-360-data-analysis}}

\subsection{Workspace View}
\label{\detokenize{mcdmp_app_ug:workspace-view}}
The Workspace is the main area of the 360 Data Analysis application page and displays the detail of the information set.

You can choose from the following views:
\begin{itemize}
\item {} 
Map view -   A geographic map overview of your data center locations.

\item {} 
Dashboard view - A detailed breakdown of the content\&apos;s metadata in a number of tiles.

\item {} 
Item List Preview - A sample of the metadata that matches your filters.

\item {} 
List views - Longer forms of the short lists that appear in the dashboard for the following entities:
\begin{itemize}
\item {} 
Locations

\item {} 
Content Sources

\item {} 
Repositories (shares)

\item {} 
Data Stores

\item {} 
Tags

\item {} 
Owners

\item {} 
Item Types

\item {} 
Item Extensions

\end{itemize}

\end{itemize}


\subsection{Dashboard}
\label{\detokenize{mcdmp_app_ug:dashboard}}
The Dashboard View is at the heart of the 360 Data Analysis application’s ability to uncover the hidden gems in your data.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{DashboardView}.png}
\end{figure}
\end{quote}

The 360 Data Analysis dashboard presents a number of tiles which summarize your data in different ways. The layout and breakdown of your information are highly customizable.
The dashboard contains three types of tiles:
\begin{itemize}
\item {} 
The Map tile - This tile displays the Map view in a smaller space.

\item {} 
The list tiles - These tiles list Locations, Content Sources, Owners, Shares, Item Types, Item Extensions, and Data Stores. See {\hyperref[\detokenize{mcdmp_app_ug:list-titles}]{\sphinxcrossref{\DUrole{std,std-ref}{List tiles}}}}.

\item {} 
The chart tiles - These tiles show the distribution of the file sizes and ages. See {\hyperref[\detokenize{mcdmp_app_ug:chart-tiles}]{\sphinxcrossref{\DUrole{std,std-ref}{Chart tiles}}}}.

\end{itemize}


\subsubsection{Changing the Dashboard layout}
\label{\detokenize{mcdmp_app_ug:changing-the-dashboard-layout}}
You can customize the dashboard layout to your preference.

\sphinxstylestrong{Moving tiles}

You can easily move any tile to another position on the dashboard by dragging the tile title in the top-left corner and dropping the tile in a new position. The other tiles arrange themselves around your new tile position.

The tiles also move around, but preserve their order (left-to-right, top-to-bottom), when the browser window width changes

\sphinxstylestrong{Tile size}

Click the hamburger icon  (in the top right of every tile), then click \sphinxstylestrong{Tile Size}.

The current tile size is highlighted with a dot. Click one of the other sizes and the tile will redraw with the new size. The other tiles move around the resized tile, preserving the general tile order.
The Map tile only supports small and large sizes.

\sphinxstylestrong{Hiding tiles}

Click the hamburger icon and select \sphinxstylestrong{Hide this tile}. The tile is removed from the page.
To restore the tile, use the Cog icon.
\begin{quote}
\phantomsection\label{\detokenize{mcdmp_app_ug:chart-tiles}}\end{quote}


\paragraph{Chart tiles}
\label{\detokenize{mcdmp_app_ug:chart-tiles}}\label{\detokenize{mcdmp_app_ug:id2}}
Item size and age are represented in the Dashboard as charts. These charts can be customized in a number of ways.

You can change the y-axis of the chart tiles, File Size, and Ages, between a linear (default) and a logarithmic scale. The latter is particularly helpful if you have a range of values which differs by several orders of magnitude.

In the following example, the size of data is in the small (0-10kB and 10-100kB) file sizes much easier to see.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{ChartLogarithmic}.png}
\end{figure}
\end{quote}

Alternatively, you can find out more about a single bar on the chart by clicking on it. On the following Age chart, click the bar to see a more accurate total size that can be read from the bar size, plus an accurate count of the number of items.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{AgePopup}.png}
\end{figure}
\phantomsection\label{\detokenize{mcdmp_app_ug:list-titles}}\end{quote}


\paragraph{List tiles}
\label{\detokenize{mcdmp_app_ug:list-tiles}}\label{\detokenize{mcdmp_app_ug:list-titles}}
The list tiles (Locations, Content Sources, Containers, Tags, Owners, Item Types, Item Extensions)  work in the same way. The tiles display a list of 7 (small or wide tile size) or 20 (tall or large tile size) entries which match your current filter criteria.

In wide and large tile sizes, both the size and the item count are displayed, graphically and numerically.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{WideListTile}.png}
\end{figure}
\end{quote}

By default, the list tiles show the entries for the largest total sizes in descending order. You can toggle between largest-descending and smallest-ascending order of size by clicking \sphinxstylestrong{Total Size}. Alternatively to can do the same for item count by clicking \sphinxstylestrong{Item Count}.

Or click the hamburger icon  (in the top right of the tile), and then click \sphinxstylestrong{Show} \textgreater{} \sphinxstylestrong{Item Count} to see the item count.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{ListCount}.png}
\end{figure}
\end{quote}

All list tiles include a link in the lower-right corner which let you navigate to a list view which lets you scroll through more entries in the list. The name of each entry in the list is a link which takes you to the Dashboard Entity page, displaying further detail for that entry.


\paragraph{Ages tiles}
\label{\detokenize{mcdmp_app_ug:ages-tiles}}
In wide or large form, the Ages tile looks like this:
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{WideAgesModLog}.png}
\end{figure}
\end{quote}

The example is a wide tile that shows the modified age on a logarithmic scale.  As with the Ages filter, the tile can display the created, modified, or last accessed ages. Click the desired age type in the top left of the tile.

The Ages tile also shows the age broken down into 4 groups, the last 30 days, the last 3 months, the last 4 quarters and totals by year.  For example, this grouping makes it easier to compare between quarters. In the  small or narrow form, the tile is too small to display all four of these groups, so it displays only one, as shown in the following figure:
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{SmallAgesY}.png}
\end{figure}
\end{quote}

The following tile shows the last-accessed age for years on a linear scale.  The \sphinxstylestrong{D M Q Y} in the top-right of the tile lets you switch between day, month, quarter, and year groups.   Click \sphinxstylestrong{M} to display the age broken into the last 3 months.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{SmallAgesM}.png}
\end{figure}
\end{quote}


\paragraph{Dashboard entities}
\label{\detokenize{mcdmp_app_ug:dashboard-entities}}
If you click on a name of an entity in a dashboard list or on the list view of Locations, Content Sources, Containers, Tags, Owners, Item Types, or Item Extensions, you are taken to a dashboard page for that entity.
For example, if you click Dallas in your \sphinxstylestrong{Locations} list, the data for only the Dallas location is displayed.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{Entity}.png}
\end{figure}
\end{quote}

A dashboard entity view differs slightly from the normal \sphinxstylestrong{Dashboard} view because the view applies to a single entity only.
\begin{itemize}
\item {} 
The breadcrumb trail in the Header shows that you are in a dashboard for the Location Dallas.

\item {} 
The \sphinxstylestrong{Locations} filter indicates that you are only showing Dallas.

\item {} 
The dashboard does not show the Locations tile. Instead, the top-left tile is fixed and shows the detailed information for the Dallas location, in this case, a small map, address and contact details.

\item {} 
An activity chart is displayed, which shows 100\% with no age filter set. You can look for hotspots of activity by adjusting the Age filters. For example, to see how much item activity has occurred in this location in the last month, go to the Ages filter, select the \sphinxstylestrong{Last Accessed} tab, and apply the 0-7 day, 7-14 day and 14-30 day criteria.

\end{itemize}

Hover over (or click) the ? icon to get information on the activity chart:
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{itemactivity}.png}
\end{figure}
\end{quote}

The accuracy of the activity is determined by the accuracy of the underlying date information for the items.

The other tiles on the dashboard entity behave as the normal Dashboard.

To leave the dashboard page for this entity use the back option, the navigation bar, or click the breadcrumb trail.


\section{Other data views}
\label{\detokenize{mcdmp_app_ug:other-data-views}}
The analytics information is presented in to you in various forms to provide different perspectives.


\subsection{Map view}
\label{\detokenize{mcdmp_app_ug:map-view}}
The Map View displays a geographic map overview of your data centers in the \sphinxstylestrong{Workspace}.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{MapView}.png}
\end{figure}
\end{quote}

\sphinxstylestrong{Locations}

Your data center locations are shown as circles on the map, with the diameter indicating the size of data present at that location, along with the location name.

Where locations are so close together that they would overlap at the current zoom level, the adjacent locations are combined into a single composite location, with the number of locations being displayed, instead of the name.

For example, if there are three locations close together in Western Europe.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{Map3Locations}.png}
\end{figure}
\end{quote}

If you hover over the location, a pop-up gives more detail.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{MapPopup}.png}
\end{figure}
\end{quote}

Click \sphinxstylestrong{Explore} to navigate to a \sphinxstylestrong{Dashboard} view, filtered to show these locations in more detail. If there is only one location, then click Explore to navigate to the \sphinxstylestrong{Dashboard Entity} view for this location.

As with all views, the data displayed represents your current filter settings. Any locations with zero total data do not appear in the Map view.

To see more detail on the map, zoom-in so that the locations are spread apart.


\subsection{Item list preview}
\label{\detokenize{mcdmp_app_ug:item-list-preview}}
Item List Preview lets you quickly review an indicative sample of the items that match your current filters. This can help you decide whether the correct filters are applied before you do further analysis or initiate a lengthy export. You can also copy the sample data into a spreadsheet for further analysis.

The items that are displayed in this view are only a subset of all the items that match the applied filters.

\sphinxstylestrong{Reviewing the sample item list}
\begin{enumerate}
\item {} 
By default, the Item List Preview  view shows up to 1000 items matching the current filter set. By default only 100 items are displayed on the Item List Preview page. To change the sample size, select a new item sample size from the Item Sample Size selector at the top-right corner of the view. The view reloads automatically when a new sample size is selected. Your filter settings may return fewer items than the sample size selected.

\item {} 
If the page is narrow or the full file path is long then the file path will be compressed to fit into the page. For more detail, hover over the file path and a pop-up will show the full path:

\item {} 
You can sort the list on any of the displayed column in either ascending or descending order by repeatedly clicking on the column header. A \textasciicircum{} indicates ascending order down the page and a v indicates descending order.

\item {} 
You can modify the current filters at any time. When you modify a filter, the view reloads the item data that matches the new filter criteria. See “Filters”

\item {} 
Click on a location or a content source to navigate to the Dashboard view for that location or content source.

\end{enumerate}

You can copy the item list data to a spreadsheet for further analysis.

\sphinxstylestrong{To copy the item list data}
\begin{enumerate}
\item {} 
On the Item List Preview, select the check box to the left of each item, or to select all rows in the sample list, select the check box at the top of the list.

\end{enumerate}
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{Select_rows}.png}
\end{figure}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
When one or more rows are selected, the Copy selected rows to clipboard button at the top of the Item List Preview table turns blue. Click the button to copy these rows.

\end{enumerate}


\subsection{List views}
\label{\detokenize{mcdmp_app_ug:list-views}}
While the \sphinxstylestrong{Dashboard} can show list tiles of up to 20 entries, you can scroll through lists of any length on the List views. You can access the List views from the navigation bar or from the List view links on Dashboard tiles.

The list views are available for the following:
\begin{itemize}
\item {} 
Locations

\item {} 
Content Sources

\item {} 
Repositories

\item {} 
Data Stores

\item {} 
Tags

\item {} 
Owners

\item {} 
Item Types

\item {} 
Item Extensions

\end{itemize}

In the figure, the list shows the totals for all the shares matching the current filter criteria. Click the column header to sort the list in ascending or descending order by Name, Total Size, or Item Count. The total number of rows is shown in the bottom left corner of the view.

The scrolling list has no size limit other than the number of rows in your data, so it can be scrolled indefinitely. The scrolling list uses an infinite scroll technique where the first thousand rows are loaded and scrollable. When you start to scroll down to near the bottom of this thousand, the next thousand are loaded and so on. The only symptom you may notice is a slight pause when the scrollbar nears the bottom of the list, followed by a sudden change in the scroll bar position as the next set of rows are added to the list.

Each of the blue values in the List view columns is a link which takes you to the Dashboard Entity page, displaying further detail for that entry.


\section{About classifying content resources}
\label{\detokenize{mcdmp_app_ug:about-classifying-content-resources}}
With the continuous growth of unstructured data in the business environment, taking decisions to archive and delete content of business or legal value is a challenge. You can simplify data remediation decisions by categorizing and organizing data based on tags and policies.

The Loom Platform helps you analyze the files that it monitors by using built-in policies to assign classification tags to files in your environment. After the files are classified, you can use the classification tags to filter the files for searches, reviews, and remediation.

The 360 Data Analysis app deployed on the Loom platform integrates with Veritas Information Classifier to help you classify files.

Use the 360 Data Analysis application to do the following:
\begin{itemize}
\item {} 
Improve content analytics by focusing on relevant and classified data set to perform risk analysis and remediation.
Classification enables you to identify the type of data being stored in repositories (for example, Personally Identifiable Information - PII), the purpose of the data, and the risk that is associated with it (whether sensitive or otherwise).

\item {} 
Make informed decisions to retain, secure, move, delete, or monitor data, and control permissions based on the classification tags.

\item {} 
Ensure that the data complies with legal requirements.

\item {} 
Categorize and tag content to make it more accessible, searchable, and usable, specifically for archiving, e-discovery, and audits.

\item {} 
View the classified data by grouping the files based on tags that are assigned to them.

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
For Loom the Alpha 2 release, 360 Data Analysis supports classification of files stored on Native File Systems, Microsoft SharePoint on-premises, and Microsoft OneDrive only.
\end{sphinxadmonition}

If native classification on Microsoft OneDrive is enabled, the files stored on OneDrive will be assigned tags based on the native classification policies. Files which are classified by the OneDrive native classification will be ignored by the Loom classification engine.


\subsection{Prerequisites for classification}
\label{\detokenize{mcdmp_app_ug:prerequisites-for-classification}}
Ensure that the following requirements are met before you initiate classification of files from the 360 Data Analysis app:
\begin{itemize}
\item {} 
The native file system that you want to classify should support SMB version 2.

\item {} 
The domain name of the connection is resolvable from the on-premises ESX Data Engine cluster.

\item {} 
Connectors are configured and scanned before initiating a classification request.

\item {} 
In the \sphinxstylestrong{Classification Engine} UI, configure policies, patterns, or tags. Enable or disable the policies, as required. If a policy is not enabled, the classification action will be initiated, but will result in an error.

\item {} 
The Loom Alpha 2 release is integrated with Veritas Information Classifier version 2.1.3 to support the classification action. For the latest help on using the classification engine, see \sphinxhref{http://veritashelpsupport.com/Welcome?locale=EN\_US\&context=VIC2.1.3}{Veritas Information Classifier Online Help}.

\end{itemize}


\subsection{Limitations of the classification feature for Alpha 2 release}
\label{\detokenize{mcdmp_app_ug:limitations-of-the-classification-feature-for-alpha-2-release}}
Note the following limitations that apply to the classification feature:
\begin{itemize}
\item {} 
The 360 Data Analysis classification feature does not support content scans of NFS shares and of cloud repositories other than Microsoft OneDrive.

\item {} 
Files that are less than one KB and more that 10 MB in size will not be sent for classification.

\item {} 
If native classification is enabled and files have been classified and assigned tags, then VIc will ignore the files that

\item {} 
Reclassification of files is not supported. If a file has been classified once, it will not considered for classification.

\item {} 
The classification engine allows you to identify Personally Identifiable Information using various built-in policies, such as France PII policy, Germany PII policy, and so on. Files that are evaluated by more than one PII policy will be assigned a tag for every PII policy that is violated.

\end{itemize}


\section{Finding your way around the Classification Policies UI}
\label{\detokenize{mcdmp_app_ug:finding-your-way-around-the-classification-policies-ui}}
The classification engine window is divided into three main areas: the navigation bar, item list, and details pane.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{vic}.svg}
\end{figure}

\sphinxstylestrong{Navigation bar}

The navigation bar provides buttons with which you can open the classification engine pages. You can collapse the bar so that only the buttons show, or pin the bar so that it remains expanded while you work.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{vic_nav_bar}.svg}
\end{figure}

\sphinxstylestrong{Item list}

The item list provides a list of the available items, together with basic information about them. Click an item to view more information in the details pane.

The controls at the top of each list let you search for items by name, filter the items according to various criteria, expand and collapse the list, and change the sort order.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{vic_item_list}.svg}
\end{figure}

\sphinxstylestrong{Details pane}

The details pane provides extensive information on the selected item. You also use this pane to edit an item or create a new one to add to the list.


\subsection{About setting classification policies}
\label{\detokenize{mcdmp_app_ug:about-setting-classification-policies}}
The Integrated Classification Engine is unified with Veritas products to  classify items based on their content and metadata. Use the classification engine to set up the following:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{policies_grayscale}.png}
\end{sphinxfigure-in-table}\relax
&
Policies. The classification engine evaluates the items that you submit for
classification against a set of policies. Each policy specifies the conditions that
an item must meet to be assigned a specific classification tag.
The numerous built-in policies cover many of the regulations and corporate
standards for which you may want to classify items, and you can create custom
policies if you have additional requirements.
\\
\hline\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{patterns_grayscale}.png}
\end{sphinxfigure-in-table}\relax
&
Patterns. Each of the built-in policies checks the items that you submit for
classification for one or more patterns. These patterns use sophisticated
algorithms to look for matches that meet the required confidence level. You can
incorporate the built-in patterns in any custom policies that you create, and also
create custom patterns of your own.
\\
\hline\begin{sphinxfigure-in-table}
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{_static/tags_grayscale}.png}
\end{sphinxfigure-in-table}\relax
&
Tags. When an item that you have submitted for classification meets the conditions
of a policy, the classification engine assigns the associated tags to the

item. You can create custom tags to add to the large number of built-in ones.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{About Policies}
\label{\detokenize{mcdmp_app_ug:about-policies}}
The classification engine evaluates the items that you submit for classification against a set of policies. Each policy specifies the conditions that the items must meet to be assigned a specific classification tag. For example, you can create a simple policy to assign the tag “Financial” to items that contain any of the terms \sphinxstyleemphasis{fraud}, \sphinxstyleemphasis{cover up}, and \sphinxstyleemphasis{write off}.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{vic_policy_example}.png}
\end{figure}

Initially, all the policies are disabled. You must enable a policy if you want the classification engine to check for and tag the items that match the policy.


\subsection{About policy conditions}
\label{\detokenize{mcdmp_app_ug:about-policy-conditions}}
A condition specifies the criteria that an item must meet for the classification engine to consider it a match. Your policies can contain any number of conditions.


\subsection{Basic components of a condition}
\label{\detokenize{mcdmp_app_ug:basic-components-of-a-condition}}
All conditions have this basic form:

\sphinxstyleemphasis{property operator value}

For example, in the following condition, “Content” is the property, “contains text” is
the operator, and “Stocks” is the value:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{VIC_condition}.png}
\end{figure}

The property specifies the part or characteristic of an item that you want to evaluate:
its content, title, modified date, file size, and so on. When you choose a property
from the list, the options in the two other fields change to suit it. For example, if you
choose the “Modified date” property, the other fields provide options with which you
can set one or more dates. For properties such as “Content”, “Title”, and “Author”,
the available operators are as follows:
\begin{itemize}
\item {} 
\sphinxstylestrong{contains text}

\item {} 
\sphinxstylestrong{matches regex}

\item {} 
\sphinxstylestrong{matches pattern}

\end{itemize}

At the right of each condition, you can specify the minimum number of times that
an item must meet the criteria for the classification engine to consider it a
match.

\sphinxstylestrong{Custom fields}

Various applications that you use in your organization may add custom property
information to the items that you want to classify. For example, when Enterprise
Vault processes an item, it populates a number of the item’s metadata properties
with information and stores this information with the archived item: the date on
which Enterprise Vault archived the item, the number of attachments that it has,
and so on.

If you know the name of a property that particularly interests you, you can enter it
as a custom field in your policy conditions.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{vic_custom_fields}.png}
\end{figure}

\sphinxstylestrong{Text matches}

Observe the following guidelines when you set up a condition to look for specific
words or phrases in the items that you submit for classification:
\begin{itemize}
\item {} 
The condition can look for multiple words or phrases, if you place each one on a line of its own. An item needs to contain just one word or phrase in the list to meet the condition.

\item {} 
Select \sphinxstylestrong{Match Case} to find only exact matches for the uppercase and lowercase characters in the specified words or phrases.

\item {} 
Select \sphinxstylestrong{String Match} to find instances where the specified words or phrases are contained within other ones. For example, if you select this option, the word \sphinxstylestrong{enter} matches enters, entertainment and carpenter. If you clear the option, \sphinxstylestrong{enter} matches only \sphinxstyleemphasis{enter}. Similarly, if you select \sphinxstylestrong{String Match}, the phrase \sphinxstylestrong{call me} matches call media and recall meeting, but not surgically mend.

\item {} 
You can place the proximity operators NEAR and BEFORE between two words in the same line. For example, \sphinxstylestrong{tax NEAR/10 reform} matches instances where there are no more than ten words between tax and reform. \sphinxstylestrong{sales BEFORE/5 report} matches instances where sales precedes report and there are no more than five words between them. The number is mandatory in both cases.

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
These proximity operators may not work as expected when evaluating formatted data, such as tables and spreadsheets. The conversion process that this data undergoes before it is classified can swap the order of the table cells. For example, suppose that a spreadsheet contains the word sales in one cell and report in the cell immediately to the right. This should match the operator \sphinxstylestrong{sales BEFORE/5 report} but may not do so after the spreadsheet has been converted, because the conversion process has transposed the two words.
\end{sphinxadmonition}
\begin{itemize}
\item {} 
Word and phrases can include the asterisk (*) and question mark (?) wildcard characters. As part of a word, an asterisk matches zero or more characters. On its own, the asterisk matches exactly one word. A question mark matches exactly one character. For example:

\end{itemize}
\begin{itemize}
\item {} 
\sphinxstylestrong{stock*} matches stock, stocks, and stockings.

\item {} 
\sphinxstylestrong{ock} matches stock and clock.

\item {} 
\sphinxstylestrong{ock} matches stock and clocks.

\item {} 
\sphinxstylestrong{??ock} matches stock and clock, but not dock.

\item {} 
\sphinxstylestrong{sell} \sphinxstylestrong{stock} matches sell the stock and sell some stock, but not sell stock. You can use  wildcards in combination with the NEAR and BEFORE operators. For example:

\item {} 
\sphinxstylestrong{s?l? BEFORE/1 stock*} matches sold the stock, sell stocks, and sale of stockings.

\end{itemize}

\sphinxstylestrong{Regular expression matches}

A regular expression, or regex for short, is a pattern of text that consists of ordinary characters (for example, letters a through z) and special characters, called
\sphinxstyleemphasis{metacharacters}. The pattern describes one or more strings to match when searching text. For example, the following regular expression matches the sequence of digits
in all Visa card numbers:

\sphinxcode{\textbackslash{}b4{[}0-9{]}\{12\}(?:{[}0-9{]}\{3\})?\textbackslash{}b}

Your regular expressions must conform to the Perl regular expression syntax.

You may find it helpful to build and test your regular expressions using the free online tool at \sphinxurl{https://regex101.com.}

This tool displays an explanation of your regular expression as you type it, and also lists all matches between the regular expression and a test string of your choice. The default regular expression flavor, \sphinxstylestrong{pcre (php)}, is compatible with the classification engine.

\begin{sphinxadmonition}{note}{Note:}
Looking for regular expression matches is considerably slower than looking for matches for specific words or phrases. You can greatly improve performance and accuracy by looking for instances where both types of matches occur in proximity to each other. To do this, set up an \sphinxstylestrong{All of} condition group that contains both a regular expression condition and a \sphinxstylestrong{contains text} condition for finding specific words and phrases, and specify the required distance within which matches must occur. The classification engine first evaluates the \sphinxstylestrong{contains text} condition and only then looks for a regular expression match.
\end{sphinxadmonition}

\sphinxstylestrong{Pattern matches}

A pattern match evaluates the selected item property against an existing Veritas Information Classifier pattern. Depending on the selected pattern, you may be able
to set the confidence levels that you are willing to accept. A high confidence level is likely to produce fewer but more relevant matches.

Note the following if you do not get the expected results when you test a policy that makes use of a built-in pattern:
\begin{itemize}
\item {} 
It is important to check that your test item meets the pattern confidence levels. For example, by default, the Credit Card Policy looks for content that matches the pattern “Credit/Debit Card Number” with medium to very high confidence. To meet the requirements of the medium confidence level, an item must contain either of the following:
\begin{itemize}
\item {} 
A delimited credit card number (one that contains spaces or dashes between the numbers).

\item {} 
Both a non-delimited credit card number and one or more credit card keywords, such as “AMEX” or “Visa”.

\end{itemize}

\end{itemize}

So, an item does not meet these requirements if it contains a non-delimited credit card number but it does not also contain credit card keywords.
\begin{itemize}
\item {} 
After you click \sphinxstylestrong{Show details} to view the results of a test, the \sphinxstylestrong{Test classification results} window may fail to highlight some or all of the matches. This is a known issue with certain patterns only. A future version of the classification engine will correct the issue.

\end{itemize}

\sphinxstylestrong{Condition groups}

You can group a set of conditions and nest grouped conditions within other grouped conditions. The group operator that you choose determines whether an item must meet all, some, or none of the conditions in the group to be considered a match.
The following group operators are available:
- \sphinxstylestrong{All of.} An item must meet all the specified conditions.
- \sphinxstylestrong{Any of.} An item must meet at least one of the specified conditions.
- \sphinxstylestrong{None of.} An item must not meet any of the specified conditions.

\begin{sphinxadmonition}{note}{Note:}
You can nest a \sphinxstylestrong{None of} group within an \sphinxstylestrong{All of} group to look for certain condition matches while also excluding others. For example, to achieve the effect of “(condition X AND condition Y) BUT NOT condition Z”, you would include the X and Y conditions in an \sphinxstylestrong{All of} group and the Z condition in a nested \sphinxstylestrong{None of} group.
\end{sphinxadmonition}
\begin{itemize}
\item {} 
\sphinxstylestrong{n or more of.} An item must meet the specified number of conditions.

\end{itemize}

For an \sphinxstylestrong{All of} group only, you can choose to look for instances where the conditions occur within a specified number of characters of each other. For example, the following condition group looks for instances where the word Goodbye appears within 20 characters of the word \sphinxstyleemphasis{Hello}:

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{VIC_proximity_search}.png}
\end{figure}

The text string “You say Goodbye and I say Hello” matches these conditions because there are fewer than 20 characters between the first character of Hello and the first character of Goodbye. Similarly, the string “You say Hello and I say Goodbye” also matches because there are fewer than 20 characters between the ends of the two words. In each case, the spaces count as characters.

\begin{sphinxadmonition}{note}{Note:}
When you conduct \sphinxstylestrong{within nn characters} proximity searches, take care not to duplicate the same search terms across multiple conditions. For example, suppose that you define one condition to look for the names Fred, Sue, and Bob, and a second to look for \sphinxstyleemphasis{Joe}, \sphinxstyleemphasis{Bob}, and \sphinxstyleemphasis{Sarah}. An item that contains a single instance of Bob would match these conditions.
\end{sphinxadmonition}

Rather than choose the \sphinxstylestrong{from the first condition} option, you can choose \sphinxstylestrong{in a sliding window}. This option looks for instances where the conditions occur within any sequence of characters of the specified number. For example, a condition group that looks for instances where the word Goodbye appears within a 20-character sliding window of the word \sphinxstyleemphasis{Hello} does not match “You say Goodbye and I say \sphinxstyleemphasis{Hello}. There are 23 characters between the start of the word Goodbye and the end of the word \sphinxstyleemphasis{Hello}.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=0.75]{{VIC_sliding_window}.svg}
\end{figure}


\subsection{About built-in policies}
\label{\detokenize{mcdmp_app_ug:about-built-in-policies}}
The built-in Information Classifier policies are arranged in the following groups.

\sphinxstylestrong{Table: Corporate Compliance}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
Authentication Policy
&
Authentication information, such as
\\
\hline
Company Confidential
and Intellectual
Property Policy
&
Documents that are confidential,
secret, or internal-only, or that
contain intellectual property source
\\
\hline
Ethics and Code of
Conduct Policy
&
Terms that are unethical or against
corporate code of conduct.
\\
\hline
IP Address Policy
&
Internet Protocol version 4 (IPv4)
and version 6 (IPv6) addresses.
\\
\hline
PCI-DSS Policy
&
Content that is subject to the Payment
Card Industry Data Security Standard
PCI-DSS), including credit and debit
card numbers.
\\
\hline
Proposals / Bids Policy
&
Corporate proposal and bid documents.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Table: Financial Regulations}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
Bank Account Number Policy
&
Country-specific or international bank account numbers.
\\
\hline
Credit Card Policy
&
Credit and debit cards.
\\
\hline
Gramm-Leach-Bliley Act (GLBA) Policy
&
Personal financial information for Financial
Services Modernization Act of 1999, also known
as Gramm-Leach-Bliley Act (GLBA) or Public Law
106-102.
\\
\hline
SWIFT Codes Policy
&
Society for Worldwide Interbank Financial Telecommunication
(SWIFT) codes, also known as Bank Identifier Codes (BIC),
Business Identifier Codes (BIC), or ISO 9362, and related
content.
\\
\hline
U.S. Financial Forms / Documents
Policy
&
U.S. financial forms and documents.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Table: Health Regulations}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
Australia Individual Healthcare
Identifier (IHI) Policy
&
Australia Individual Healthcare Identifiers (IHI) and
related content.
\\
\hline
Canada Healthcare Identifiers Policy
&
Canada Healthcare Identifiers and related content.
\\
\hline
ICD 10 CM Diagnosis Indexes Policy
&
ICD 10 CM diagnosis indexes (textual names).
\\
\hline
Medical Record Number Policy
&
Medical record numbers (generically).
\\
\hline
U.S. Drug Enforcement Agency (DEA)
Number Policy
&
U.S. Drug Enforcement Agency (DEA) numbers and related
content.
\\
\hline
U.S. Health Insurance Portability and
Accountability Act (HIPAA) Policy
&
Electronic patient health information (ePHI)
information for United States Health Insurance
Portability and Accountability Act (HIPAA) , also
known as Public Law 104-191.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Table: International Regulations}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
Australia Drivers License Number Policy
&
Australian driver’s license numbers
\\
\hline
Australia Passport Policy
&
Australian passport numbers.
\\
\hline
Australia Tax Policy
&
Australian tax file number and related content.
\\
\hline
Canada Drivers License Number Policy
&
Canadian driver’s license numbers.
\\
\hline
Canada Passport Policy
&
Canadian passport numbers.
\\
\hline
Canada Social Insurance Number Policy
&
Canadian social insurance numbers.
\\
\hline
France National ID Policy
&
French National Identifiers and related content.
\\
\hline
Italy Codice Fiscale Policy
&
Italian Codice Fiscale numbers, also known as Italian fiscal
code card numbers.
\\
\hline
Switzerland National ID Policy
&
Swiss National Identifiers and related content.
\\
\hline
U.K. Drivers License Number Policy
&
U.K. driver’s license numbers and related content.
\\
\hline
U.K. National ID Policy
&
U.K. National Identifiers and related content.
\\
\hline
U.K. National Insurance Number (NINO)
Policy
&
U.K. National Insurance number and related content.
\\
\hline
U.K. Passport Number Policy
&
U.K. passport number and related content.
\\
\hline
U.K. Unique Tax Reference (UTR) Policy
&
U.K. Unique Tax Reference (UTR) number and related content.
\\
\hline
U.S. Drivers License Number Policy
&
U.S. driver’s license numbers.
\\
\hline
U.S. Passport Policy
&
U.S. passport and passport card numbers.
\\
\hline
U.S. Social Security Number (SSN) and
Taxpayer ID Policy
&
U.S. taxpayer identification numbers. This is typically
the U.S. Social Security Number (SSN).
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Table: Personally Identifiable Information}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
France Personal Data Policy
&
Personal data applicable to France’s Federal and State laws,
including France’s Act No 78-17 on Information Technology,
Data Files and Civil Liberties (DPA), European Privacy
Directive 95/46/EC on data protection (Data Protection
Directive), and the European General Data Protection
Regulation (GDPR).
\\
\hline
Germany Personal Data Policy
&
Personal data applicable to Germany’s Federal and State laws,
including Germany’s Federal Data Protection Act (BDSG),
European Privacy Directive 95/46/EC on data protection (Data
Protection Directive), and the European General Data
Protection Regulation (GDPR).
\\
\hline
Italian Personal Data Policy
&
Personal data applicable to Italy’s Legislative Decree No.
196/2003, which contains the Italian Personal Data
Protection Code (Code), European Privacy Directive 95/46/EC on
data protection (Data Protection Directive), and the European
General Data Protection Regulation (GDPR).
\\
\hline
Spain Personal Data Policy
&
Personal data applicable to Spain’s Data Protection Act (Law
15/1999 on the protection of personal data), European
Privacy Directive 95/46/EC on data protection (Data
Protection Directive), and the European General Data
Protection Regulation (GDPR).
\\
\hline
Turkey Personal Data Policy
&
Personal data applicable to Turkey’s Law on Protection of
Personal Data No 6698 (Data Protection Law, KVKK),
European Privacy Directive 95/46/EC on data protection
Data Protection Directive), and the European General Data
Protection Regulation (GDPR).
\\
\hline
United Kingdom Personal Data Policy
&
Personal data applicable to United States’s Federal and
State laws, including The Federal Trade Commission Act
(FTC Act), the Financial Services  Modernization Act
(Gramm-Leach-Bliley Act GLB/GLBA)), the Health Insurance
Portability and Accountability Act (HIPAA), the Fair Credit
Reporting Act (FCRA), the Fair and Accurate Credit
Transactions Act (FACTA), among many others.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstylestrong{Table: U.S. Federal Regulations}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Policy
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Detects
\unskip}\relax \\
\hline
Criminal History Policy
&
Identity history summary (Criminal History Record,
or Rap sheet) for Criminal Justice Systems.
\\
\hline
Fair Credit Reporting Act (FCRA) Policy
&
Personal credit information for Fair Credit
Reporting Act (FCRA).
\\
\hline
Family Educational Rights and Privacy
Act (FERPA) Policy
&
Educational content subject to the Family
Educational Rights and Privacy Act (FERPA).
\\
\hline
Federal Financial Institutions
Examination Council (FFIEC) Policy
&
Personally identifiable and financial information for
the Federal Financial Institutions Examination
Council (FFIEC).
\\
\hline
Federal Information Security
Management Act (FISMA) Policy
&
Personal and security information and for Federal Information
Security Management Act (FISMA) of 2002, also known as
E-Government Act of 2002, also known as Public  Law 107-347.
The type of information that should be protected depends
on the relevant agency/sector.
\\
\hline
U.S. Internal Revenue Service (IRS)
1075 Policy
&
IRS tax forms and financial information for the U.S.
Internal Revenue Service (IRS) 1075 and Internal
Revenue Code (IRC) 6103.
\\
\hline
U.S. Securities and Exchange
Commission (SEC) Forms Policy
&
U.S. Securities and Exchange Commission (SEC)
Forms.
\\
\hline
California Assembly Bill 1298 (HIPAA)
Policy
&
Electronic patient personally identifiable information
(PII) for California Assembly Bill 1298
(California/CA AB 1298).
\\
\hline
California Financial Information Privacy
Act (SB1) Policy
&
Personal financial information for California
Financial Information Privacy Act, also known as
CA SB1.
\\
\hline
Massachusetts Regulation 201 CMR
17.00 (MA 201 CMR 17) Policy
&
U.S.- and Massachusetts- centric personally identifiable
information in accordance with Massachusetts Regulation 201
CMR 17.00.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Creating and editing policies}
\label{\detokenize{mcdmp_app_ug:creating-and-editing-policies}}
The classification engine comes with a large number of built-in policies, but you can create custom policies if the built-in ones do not meet your needs.

You can also edit existing policies. However, in the case of the built-in policies, changes that you can make are quite limited.

\sphinxstylestrong{To create or edit a policy}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Policies}.

\item {} 
Do one of the following:

\end{enumerate}
\begin{itemize}
\item {} 
To create a policy from the beginning, click \sphinxstylestrong{New}.

\item {} 
To create a policy by copying an existing one, select the policy and then click \sphinxstylestrong{Copy}.

\item {} 
To edit an existing policy, select the policy and then click \sphinxstylestrong{Edit}.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Set the fields as follows:

\end{enumerate}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxstylestrong{Name}
&
Specifies the policy name. The name must be unique, and it
can contain up to 100 alphanumeric, space, and special
characters.
\\
\hline
\sphinxstylestrong{Status}
&
Enables or disables the policy. You must enable the policy if
you want the classification engine to check for and
tag the items that match the policy.
\\
\hline
\sphinxstylestrong{Description}
&
(Optional.) Provides a short description of the policy for
display in the classification engine.
\\
\hline
\sphinxstylestrong{Tags}
&
Nominates one or more tags that you want to apply to the items
that match the policy conditions. Click the \sphinxstylestrong{Tags} field to
choose from a list of the available tags.
\\
\hline
\sphinxstylestrong{Conditions}
&
Specifies one or more conditions that an item must meet for
the classification engine to consider it a match.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

4. Test the policy by clicking \sphinxstylestrong{Browse} and then choosing an item that ought to
match it.

After a few moments, the classification engine indicates whether it has found a match. When this is the case, you can click \sphinxstylestrong{Show details} to see
the matching text and confidence levels.
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Click \sphinxstylestrong{Save}.

\end{enumerate}


\subsection{Deleting policies}
\label{\detokenize{mcdmp_app_ug:deleting-policies}}
You cannot delete the built-in policies, but you can delete any custom policies that you have created.

\sphinxstylestrong{To delete a policy}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Policies}.

\item {} 
Select the policy that you want to delete and then click \sphinxstylestrong{Delete}.

\item {} 
Click \sphinxstylestrong{Yes} to confirm that you want to delete the policy.

\end{enumerate}


\subsection{Enabling or disabling policies}
\label{\detokenize{mcdmp_app_ug:enabling-or-disabling-policies}}
Initially, all the policies are disabled. You must enable a policy if you want the classification engine to check for and tag the items that match the policy.

\begin{sphinxadmonition}{note}{Note:}
Enabling a lot of policies can affect performance. In addition, policies with complex conditions take longer to process than those with simple conditions.
\end{sphinxadmonition}

\sphinxstylestrong{To enable or disable a policy}
\begin{quote}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Policies}.

\item {} 
Select one or more policies that you want to enable or disable and then click \sphinxstylestrong{Edit}.

\end{enumerate}

You can enable or disable multiple policies at once.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
In the Status field, select \sphinxstylestrong{Enabled} or \sphinxstylestrong{Disabled}.

\item {} 
Click \sphinxstylestrong{Save}.

\end{enumerate}
\end{quote}


\subsection{Exporting or importing policies}
\label{\detokenize{mcdmp_app_ug:exporting-or-importing-policies}}
The Information Classifier is available for use with multiple products. If you have several of these products and want to distribute the same policies across them all, you can export the policies from one instance of the Information Classifier and then import them into the others. The format in which the Information Classifier exports the policies is JavaScript Object Notation (JSON), an industry-standard format for exchanging data in a readable form.

You cannot export or import the built-in policies, but you can export or import any custom policies that you have created.

\sphinxstylestrong{To export a policy}
\begin{quote}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Policies}.

\item {} 
Select one or more policies that you want to export and then click \sphinxstylestrong{Export}.

\end{enumerate}

Any custom patterns and tags that you have associated with the policies will automatically be exported as well.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Save the exported JSON file.

\end{enumerate}
\end{quote}

\sphinxstylestrong{To import a policy}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Policies}.

\item {} 
Click \sphinxstylestrong{Import}.

\item {} 
Select the JSON file that you want to import.

\end{enumerate}


\subsection{About patterns}
\label{\detokenize{mcdmp_app_ug:about-patterns}}
In the conditions of a policy, you can instruct the classification engine to look for one or more patterns in the items that it classifies. For example, here are the conditions for the built-in policy that is called “Ethics and Code of Conduct Policy”:

Each condition looks for a match between the content of an item and an existing pattern: either “Individual Communication” or “Ethics \& Code of Conduct”. When an item matches both patterns, it meets the conditions of the policy.

The built-in patterns that come with the classification engine use sophisticated algorithms to look for pattern matches and assign a confidence level. You can view the range of confidence levels for a pattern by selecting it in the item list. For example, the pattern “Credit/Debit Card Number” matches with low confidence if it finds a string of digits that conform to the format of a credit card number, but it matches with high confidence if these digits are accompanied by credit-related keywords like “AMEX” and “Visa”. When you create or edit policies, you can set the required confidence level for these pattern matches.

Each built-in pattern is used by at least one of the built-in policies, and you can incorporate the patterns in any custom policies that you create. You can also create custom patterns if the built-in ones do not meet your needs. However, it is important to note that these custom  patterns are not as sophisticated as the built-in ones.

You can edit and delete custom patterns, but you cannot edit and delete built-in patterns.


\subsection{Creating or editing patterns}
\label{\detokenize{mcdmp_app_ug:creating-or-editing-patterns}}
You cannot edit the built-in patterns, but you can edit any custom patterns that you
have created.

\sphinxstylestrong{To create or edit a pattern}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Patterns}.

\item {} 
Do one of the following:

\end{enumerate}
\begin{itemize}
\item {} 
To create a pattern, click \sphinxstylestrong{New}.

\item {} 
To edit an existing pattern, select it and then click \sphinxstylestrong{Edit}.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Set the fields as follows:

\end{enumerate}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxstylestrong{Name}
&
Specifies the pattern name. The name must be unique, and it can
contain up to alphanumeric, space, and special characters.
\\
\hline
\sphinxstylestrong{Description}
&
(Optional.) Provides a short description of the pattern for
display in the classification engine.
\\
\hline
\sphinxstylestrong{Type}
&
Specifies the pattern type.
For a \sphinxstylestrong{Text} or \sphinxstylestrong{Regular expression} pattern, you must specify
the value for which to look. The same guidelines that you must
observe when you enter these values in a policy condition apply
when you enter them as a pattern value.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Test the pattern by clicking Browse and then choosing a document that ought to match it. After a few moments, the classification engine indicates whether it has found a match. When this is the case, you can click Show details to see the matching text and confidence levels.

\item {} 
Click \sphinxstylestrong{Save}.

\end{enumerate}


\subsection{Exporting or importing patterns}
\label{\detokenize{mcdmp_app_ug:exporting-or-importing-patterns}}
Just as you can export your custom policies and tags from one Information Classifier environment and then import them into another, you can also export and import custom patterns. You cannot export or import the built-in patterns.

\sphinxstylestrong{To export a pattern}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Patterns}.

\item {} 
Select one or more patterns that you want to export and then click \sphinxstylestrong{Export}.

\item {} 
Save the exported JSON file.

\end{enumerate}

\sphinxstylestrong{To import a policy}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Patterns}.

\item {} 
Click \sphinxstylestrong{Import}.

\item {} 
Select the JSON file that you want to import.

\end{enumerate}


\subsection{Deleting patterns}
\label{\detokenize{mcdmp_app_ug:deleting-patterns}}
You can delete the custom patterns that you have created, provided that they are not in use in any policies. You cannot delete the built-in patterns.

\sphinxstylestrong{To delete a pattern}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Patterns}.

\item {} 
Select the pattern that you want to delete and then click \sphinxstylestrong{Delete}.

\item {} 
Click \sphinxstylestrong{Yes} to confirm that you want to delete the pattern.

\end{enumerate}


\subsection{About tags}
\label{\detokenize{mcdmp_app_ug:about-tags}}
Every classification engine policy is associated with one or more tags. When an item that you have submitted for classification matches the conditions of a policy, the classification engine assigns the associated tags to the item. For example, the classification engine assigns the tag “Corporate-Ethics” to items that match the built-in policy “Ethics and Code of Conduct Policy”.

The classification engine comes with a large number of built-in tags, but you can create custom tags if the built-in ones do not meet your needs.


\subsection{Creating or editing tags}
\label{\detokenize{mcdmp_app_ug:creating-or-editing-tags}}
The classification engine comes with a large number of built-in tags, but you can create custom tags if the built-in ones do not meet your needs.
You cannot edit the built-in tags, but you can edit the descriptions of the custom tags.

\sphinxstylestrong{To create or edit a tag}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Tags}.

\item {} 
Do one of the following:

\end{enumerate}
\begin{itemize}
\item {} 
To create a tag, click \sphinxstylestrong{New}.

\item {} 
To edit an existing tag, select it and then click \sphinxstylestrong{Edit}.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Set the fields as follows:

\end{enumerate}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline

\sphinxstylestrong{Tag}
&
Specifies the tag name. The name must be unique, and it can
contain up to 30 alphanumeric, space, and special characters.
However, the name must not include the following characters:
\& : / \% + \textless{} \textgreater{} ?
If you are editing an existing tag, you cannot change its name.
\\
\hline
\sphinxstylestrong{Description}
&\begin{description}
\item[{(Optional.) Provides a short description of the tag for display}] \leavevmode
in the classification engine.

\end{description}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Click \sphinxstylestrong{Save}.

\end{enumerate}


\subsection{Exporting and importing tags}
\label{\detokenize{mcdmp_app_ug:exporting-and-importing-tags}}
Just as you can export your custom policies and patterns from one Information Classifier environment and then import them into another, you can also export and import custom tags. You cannot export or import the built-in tags.

\sphinxstylestrong{To export a tag}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Tags}.

\item {} 
Select one or more tags that you want to export and then click \sphinxstylestrong{Export}.

\item {} 
Save the exported JSON file.

\end{enumerate}

\sphinxstylestrong{To import a tag}
\begin{enumerate}
\item {} 
At the left of the Information Classifier, click \sphinxstylestrong{Tags}.

\item {} 
Click \sphinxstylestrong{Import}.

\item {} 
Select the JSON file that you want to import.

\end{enumerate}


\subsection{Deleting tags}
\label{\detokenize{mcdmp_app_ug:deleting-tags}}
You cannot delete the built-in tags, but you can delete any custom tags that you have created. However, you must first ensure that no policies use these tags.

\sphinxstylestrong{To delete a tag}
\begin{enumerate}
\item {} 
At the left of the classification engine, click \sphinxstylestrong{Tags}.

\item {} 
Select the tag that you want to delete and then click \sphinxstylestrong{Delete}.

\item {} 
Click \sphinxstylestrong{Yes} to confirm that you want to delete the tag.

\end{enumerate}


\section{Initiating classification of files}
\label{\detokenize{mcdmp_app_ug:initiating-classification-of-files}}
360 Data Analysis lets you scope the repositories that you want to classify. Files in the classification request are evaluated against a set of built-in and custom policies
and matching tags are assigned to these files. Note that every policy can have multiple tags and the same tags may be reused in different policies.
Thus, you may see multiple files being assigned the same tags even if they are being evaluated against different policies.

\sphinxstylestrong{To initiate a classification request:}
\begin{enumerate}
\item {} 
Log in to the 360 Data Analysis app as a Tenant Admin or a Tenant User. If you have logged in to the Loom UI, use the Application Switcher to navigate to the 360 Data Analysis app.

\item {} 
In the left-navigation pane, click \sphinxstylestrong{Classification Engine}.

\item {} 
Configure the classification policies and enable them as required.

\item {} 
Use filters to determine the scope of classification. From the configured content sources, filter the content sources to classify.

\item {} 
Further narrow down the scope by selecting the repositories (shares) that you want to classify.

\item {} 
Click \sphinxstylestrong{Views} \textgreater{} \sphinxstylestrong{Repositories}. On the \sphinxstylestrong{Repositories} page, select the resources that you want to evaluate using classification policies.

\item {} 
Click \sphinxstylestrong{Actions} \textgreater{} \sphinxstylestrong{Classify}.

\item {} 
The confirmation message provides details of the number of files sent for classification.

\end{enumerate}
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{access_mcdmp_apps}.png}
\end{figure}
\end{quote}


\subsection{Viewing the status of classification requests on the Loom Platform UI}
\label{\detokenize{mcdmp_app_ug:viewing-the-status-of-classification-requests-on-the-loom-platform-ui}}
Review the status of the classification request in the Loom UI.

\sphinxstylestrong{To view the status of classification requests}
\begin{enumerate}
\item {} 
Use the Application Switcher select \sphinxstylestrong{Monitoring} to navigate to the Job status page on the Loom UI.

\item {} 
The \sphinxstylestrong{Jobs} page displays the following information:

\end{enumerate}
\begin{quote}
\begin{itemize}
\item {} 
A unique ID assigned to the classification job request. Click the Job ID to review the following details:
\begin{itemize}
\item {} 
The number of files already classified.

\item {} 
The number of files for which classification is in-progress.

\item {} 
The number of files identified as sensitive. Loom flags a file as sensitive when its contents matches one or more policy definitions.

\end{itemize}

\item {} 
The status of the classification request. The possible states are In-progress, Completed, or Failed.

\item {} 
The type of job. For the Loom Alpha 2 release, only classification jobs are listed.

\item {} 
The priority assigned to the job. For Alpha 2 release, the priority of the on-demand classification job will always be high.

\item {} 
The start and end date of the job.

\end{itemize}
\end{quote}


\chapter{Veritas Connection Center User Guide}
\label{\detokenize{mcdmp_app_ug:veritas-connection-center-user-guide}}
Organizations use multiple content platforms for storing and retrieving their business data. 360 Data Analysis enables you to connect to and visualize the data stored across various content sources and provides a consolidated insight into your heterogeneous data estate.

Veritas Connection Center (VCC) is a common configuration and data store management portal. It is integrated with the Loom platform and can be launched from the application switcher. You can seamlessly authenticate with VCC using the Loom credentials. It serves as an administration console to configure a wide range of on-premises, cloud, and Veritas-integrated data stores, which in turn enables consumer applications, such as 360 Data Analysis to provide visibility into these additional content sources. Veritas Connections Center comprises the following two tabs:
\begin{itemize}
\item {} 
Connections - Create and edit the connections to the content source.

\item {} 
Credentials - Create and edit the credentials needed to enable the Loom platform to access the content source.

\end{itemize}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Console_VCC}.png}
\end{figure}

\begin{sphinxShadowBox}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id17}}{\hyperref[\detokenize{mcdmp_app_ug:getting-started-with-veritas-connection-center}]{\sphinxcrossref{Getting started with Veritas Connection Center}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id18}}{\hyperref[\detokenize{mcdmp_app_ug:configuring-on-cloud-content-sources}]{\sphinxcrossref{Configuring On-cloud content sources}}}

\item {} 
\phantomsection\label{\detokenize{mcdmp_app_ug:id19}}{\hyperref[\detokenize{mcdmp_app_ug:configuring-on-premises-content-sources}]{\sphinxcrossref{Configuring On-premises content sources}}}

\end{itemize}
\end{sphinxShadowBox}


\bigskip\hrule\bigskip



\section{Getting started with Veritas Connection Center}
\label{\detokenize{mcdmp_app_ug:getting-started-with-veritas-connection-center}}

\subsection{Finding your way around VCC}
\label{\detokenize{mcdmp_app_ug:finding-your-way-around-vcc}}
The Veritas Connection Center window is divided into three main areas: the navigation bar, the item list, and the details pane.


\subsubsection{Navigation bar}
\label{\detokenize{mcdmp_app_ug:navigation-bar}}
The navigation bar provides buttons with which you can open the VCC pages. You can collapse the bar so that only the buttons show, or pin the bar so that it remains expanded while you work.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Nav_pane}.png}
\end{figure}
\end{quote}


\subsubsection{Item list}
\label{\detokenize{mcdmp_app_ug:item-list}}
The item list provides a list of the available items, such as the connections or the credentials, together with basic information about them. Click an item to view more information in the details pane.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Item}.png}
\end{figure}
\end{quote}


\subsubsection{Details pane}
\label{\detokenize{mcdmp_app_ug:details-pane}}
The details pane provides extensive information on the selected item. You also use this pane to edit an item or create a new one to add to the list.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{details_pane}.png}
\end{figure}
\end{quote}


\subsection{Supported connectors}
\label{\detokenize{mcdmp_app_ug:supported-connectors}}
The Loom platform supports the following data stores and Veritas-integrated connectors that enable you to discover data across the various content platforms.

\begin{sphinxadmonition}{note}{Note:}
For Early Beta, cloud content sources- Google Cloud Storage, Microsoft Azure, Microsoft SharePoint Online, Microsoft OneDrive, Box, and on-premises content sources- Native File Server content sources and Microsoft SharePoint On-premises are supported.
\end{sphinxadmonition}


\subsubsection{Cloud connectors}
\label{\detokenize{mcdmp_app_ug:cloud-connectors}}\begin{itemize}
\item {} 
Box for Enterprise

\item {} 
Google Cloud Storage

\item {} 
Google Drive

\item {} 
GMail

\item {} 
Microsoft Azure

\item {} 
Microsoft Exchange Online

\item {} 
Microsoft OneDrive

\item {} 
Microsoft SharePoint Online

\end{itemize}


\subsubsection{On-Premise Connectors}
\label{\detokenize{mcdmp_app_ug:on-premise-connectors}}\begin{quote}

\sphinxstylestrong{Table 1-1 Supported versions}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Connector
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Supported version
\unskip}\relax \\
\hline
IBM FileNet
&
Versions 5.1
\\
\hline
OpenText Documentum
&
Versions 6.7
\\
\hline
OpenText LiveLink
&
Versions 10.5
\\
\hline
EMC Isilon
&
OneFS version 7.1.0.6 or higher
\\
\hline
Generic NFS
&
Not applicable
\\
\hline
Hitachi NAS
&
Hitachi NAS 12.x
\\
\hline
NetApp Cluster
&
CIFS - ONTAP 8.2.x or higher
NFS - ONTAP 8.2.X or and ONTAP 8.3.1 or higher
\\
\hline
NetApp Standalone
&
7.3.5 or higher
\\
\hline
Windows File Server (CIFS)
&
Windows Server 2008, or 2008 R2, 32 bit and 64 bit
Windows Server 2012, or 2012 R2 64 bit
Windows Server 2016, or 2016 R2 64 bit
\\
\hline
Veritas File System
&
6.0.1 or higher, configured in standalone or clustered
mode  using Cluster Server (VCS)
For VCS support, Clustered File System (CFS) is
not supported.
\\
\hline
Microsoft Exchange On-Premise
&
Exchange 2013 and Exchange 2016 running the latest
service pack.
\\
\hline
Microsoft SQL Server
&
SQL Server 2008
SQL Server 2012
SQL Server 2016
\\
\hline
Microsoft SharePoint
&
Microsoft SharePoint Server 2016
\\
\hline
Oracle Database
&
Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 -
64-bit Production
Oracle Database 11g Standard Edition 64-bit Production
Oracle Database 10g Express Edition Release 10.2.0.1.0 -
Production
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}
\end{quote}


\subsection{Requirements and prerequisites}
\label{\detokenize{mcdmp_app_ug:requirements-and-prerequisites}}\label{\detokenize{mcdmp_app_ug:requirements-generic}}
Before configuring a data store for monitoring, ensure that the following requirements are met:
\begin{itemize}
\item {} 
The connector machine and the on-premises data stores are part of the same domain or are in the same trusted domains.

\item {} 
The connector machine is able to successfully communicate with Veritas Connection Center (VCC).

\item {} 
In case of on-premises data stores, the on-premises connectors are installed and registered with the Connector Framework.

\item {} 
In case of cloud data stores, configure your cloud accounts to allow access to the Veritas Managed Cloud Agent.

\item {} 
The connector service port is accessible and used by a single service.

\item {} 
In case of on-premises data stores, make sure that the connector machine is geographically close to the data store being monitored.

\item {} 
The following browsers and versions are supported for VCC:
\begin{itemize}
\item {} 
Google Chrome, version 43.0 and later

\item {} 
Mozilla Firefox, version 34.0 and later

\end{itemize}

\item {} 
The fully qualified domain name (FQDN) address that you specify when adding a connection must be resolvable from the Connector node.

\end{itemize}

In addition to these requirements, ensure that you complete the connection-specific requirements before you add the connection in Veritas Connection Center.

See {\hyperref[\detokenize{mcdmp_app_ug:prereq-google-cloud-storage}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites for configuring data collection from Google Cloud Storage}}}}

See {\hyperref[\detokenize{mcdmp_app_ug:prerequisites-azure}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites for configuring data collection from Microsoft Azure Storage}}}}

See {\hyperref[\detokenize{mcdmp_app_ug:prereq-nfs}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites for configuring data collection from Native File Server connections}}}}

See {\hyperref[\detokenize{mcdmp_app_ug:prereq-sharepoint-onpremises}]{\sphinxcrossref{\DUrole{std,std-ref}{Setting up the discovery of SharePoint site collections}}}}


\subsection{High-level workflow for setting up connections to data stores from the Veritas Connection Center}
\label{\detokenize{mcdmp_app_ug:high-level-workflow-for-setting-up-connections-to-data-stores-from-the-veritas-connection-center}}
Ensure that the following prerequisites are met:
\begin{enumerate}
\item {} 
In case of on-premises content sources, setup the VMWare On-Premises Data Engine.

\item {} 
In case of cloud data stores, configure your cloud accounts to allow access to the Veritas Managed Cloud Agent.

\item {} 
Launch the Veritas Connection Center application and setup a connection to the data store using the required credentials.

\item {} 
Content sources, once discovered, are created in a disabled state by default. Launch the 360 Data Analysis app and enable the content sources.

\item {} 
Visualize your data from 360 Data Analysis application.

\end{enumerate}


\subsection{Accessing Veritas Connection Center}
\label{\detokenize{mcdmp_app_ug:accessing-veritas-connection-center}}
The Veritas Connection Center (VCC) is accessible to 360 Data Analysis app users with Tenant admin and Tenant user privileges from the application manager hosted from within the Loom UI.

\sphinxstylestrong{To launch the Veritas Connection Center}
\begin{enumerate}
\item {} 
Log in to the Loom platform URL as a Tenant admin or a Tenant user.

\item {} 
Click \sphinxstylestrong{Connectors}. The application opens.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{navigate_vcc}.png}
\end{figure}
\end{quote}

\end{enumerate}

You can now add credentials and connections for content sources that you want to monitor.

Use the application switcher to move between the Loom Platform UI and other applications, such as 360 Data Analysis.
\begin{quote}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Applictaion_Switcher}.png}
\end{figure}
\end{quote}

\begin{sphinxadmonition}{note}{Note:}
The application switcher will not be displayed for non-administrator users.
\end{sphinxadmonition}


\subsection{Content sources and hierarchies}
\label{\detokenize{mcdmp_app_ug:content-sources-and-hierarchies}}
The different data levels discovered for each data store are represented as content sources and content repositories in 360 Data Analysis.
Data stores and hierarchies summarizes the data stores and the object hierarchy.
\begin{quote}

\sphinxstylestrong{Table: Content source and hierarchies}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{3}{\X{1}{3}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Data Store
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Content Source
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Content Container
\unskip}\relax \\
\hline
Box for Enterprise
&
Box for Enterprise account
&
User Box drives
\\
\hline
Amazon S3
&
S3 buckets
&
S3 buckets
\\
\hline
Google cloud Storage
&
Regional bucket
&
Bucket
\\
\hline
Microsoft Azure
&
Azure storage account
&\begin{itemize}
\item {} 
Blob container

\item {} 
File share

\end{itemize}
\\
\hline
Microsoft OneDrive
&
OneDrive Enterprise Account
(Office 365 Account)
&
User drives
\\
\hline
OpenText Documentum
&
URL of the content management
servers
&
Repository
\\
\hline
OpenText Livelink
&
URL of the content management
servers
&
Repository
\\
\hline
IBM FileNet
&
URL of the content management
servers
&
Repository
\\
\hline
Native File Servers
&
Fully Qualified Domain Name
(FQDN) of the file server name
&
UNC share path or
NFS export path
\\
\hline
Microsoft Exchange
&
Mailbox name fetched from
the Active Directory
&
User primary email address
\\
\hline
Microsoft SQL Server
&
Server instance
&
Database
\\
\hline
Microsoft SharePoint
&
Web application name/site
collection
&
Site/Document library
\\
\hline
Oracle Database
&
FQDN of the server instance
&
Database
\\
\hline
Backup Exec
&
FQDN of the Backup Exec server
&
File system volume or
UNC share name
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsection{Terminology and basic concepts}
\label{\detokenize{mcdmp_app_ug:terminology-and-basic-concepts}}
This section describes the terminology used within the Veritas Connection Center.
\begin{quote}

\sphinxstylestrong{Table} List of terms and their description


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Term
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Data store type
&
It is a specific type of content source that provides data
for 360 Data Analysis. It may be hosted in the cloud, can
be on premises, or can also be an application that provides
the content.
\\
\hline
Data store instance
&
A specific instance of a data store type; it can be
in the cloud or on premises.
You can configure instances of the data store types
that you want to monitor for visibility or classification.
\\
\hline
VCC
&
An acronym for Veritas Connection Center.
An application that handles the creation and modification
of connections, and configuration of credentials
and schedule. This information is stored in the Connector
Framework configuration database.
\\
\hline
Connectors
&
The software components that understand the hierarchy of
data and performs the discovery and scan to collect
metadata from the data stores/connections. It is essential
to configure a connector for every data store that you
want to monitor.
A connector by itself is useless. You must create a
connection for a data store, such as, Google Cloud Storage
to be able to obtain the data. The same connector can be
used for multiple  connections.
\\
\hline
Consumer
&
Applications that want to perform operations on data
stores/connections such as, discovery, scans,
classification, archive, delete.
360 Data Analysis is an example of a consumer.
\\
\hline
Connector Framework (CF)
&
A shareable framework for executing discovery and control
jobs against extensible data stores and applications on
behalf of consumers such as 360 Data Analysis.
The Connector Framework (CF) is responsible for scheduling
and  queuing jobs among various CF nodes, processing the
results of the scan jobs and performing differentials
between two subsequent full scans, and uploading content
to the consumer(s).
\\
\hline
CF node
&
A host for the CF software such as a virtual machine,
whether deployed in the cloud or on-premises.
Each node has all CF services and data connectors
installed.
If required for scaling, a particular CF service node can
be configured with only the required CF services and data
connector.
\\
\hline
Connection
&
A configured data store instance within VCC.
It uses a connector, credentials, a data store instance,
a CF Group, schedule, and any other connector-specific
value, and is used by the Loom platform to collect metadata
for the data store instance. It is possible to have
many connections accessing the same data store instance.
\\
\hline
Group
&
A unit of deployment for resource management.
A group represents a collection of CF Node(s) and the
data store(s) that they are responsible for processing.
Customers manage their own groups for on-premise
connectors. For Cloud connectors, service groups are
created based on the region (e.g. US, Frankfurt) and
managed by Veritas.
A group is used to geographically and visually separate
data for analysis, reporting, access controls, and policy
evaluation.
However, you can create groups based on your business
requirements.
\\
\hline
Veritas-managed cloud agent
&
Refers to a CF running in the cloud that is managed
by Veritas.
The cloud agent is used to service connections configured
for cloud data stores.
\\
\hline
Jobs
&
Manage the orchestration of work in the CF.
Jobs are of 2 types, Consumer jobs and Instance jobs.
The Consumer jobs are created by the Consumer Service for
content that is enabled for collection.
The Instance jobs are internal to the CF and are created
by the Scheduler Service to best optimize collection.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Requirements for the shared storage for Agent nodes}
\label{\detokenize{mcdmp_app_ug:requirements-for-the-shared-storage-for-agent-nodes}}
Ensure that the shared volume used by the Agent nodes to store item and asset metadata adhere to the following requirements:
\begin{itemize}
\item {} 
The shared volume meets the prescribed system requirements.

\item {} 
The shared volume is a CIFS share; it can be an UNC path.

\item {} 
Ensure that the Administrator has full file-level and share-level permission on the shared location.

\item {} 
The Agent computer account also has full file-level and share-level permission on the shared location. If the Agent machines which are part of cluster, all computer accounts should have Full ACL and Full share-level permissions.
\begin{quote}
\phantomsection\label{\detokenize{mcdmp_app_ug:configuring-credentials}}\end{quote}

\end{itemize}


\subsection{Configuring credentials in VCC}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-in-vcc}}\label{\detokenize{mcdmp_app_ug:configuring-credentials}}
You can store account credentials for the data stores (connections) that enable the agent to access data from the data store accounts.

The authentication credentials can be stored in a central credential store and referenced when configuring connections in the VCC. Saving credentials simplifies the management of changes to the account user name and passwords.

For cloud data stores, such as Box and Microsoft OneDrive, you do not need to add credentials in VCC. The authentication for these data stores is controlled using the OAuth 2.0 workflow for each connector. For these data stores, you are redirected to the respective tenant accounts for authorization, where you must perform an interactive registration to acquire the authentication tokens.

Once registered the connector acquires two tokens that are used to authenticate itself to the Web APIs:
\begin{itemize}
\item {} 
The Access token that is valid for 60 minutes; it can be exchanged for a refresh token when it expires.

\item {} 
The Refresh token that is valid for 60 days. In case of Microsoft OneDrive, the Refresh token is valid only for 14 days.

\end{itemize}

The credentials are grouped in the following categories:
\begin{itemize}
\item {} 
Cloud Connectors

\item {} 
On-premises Connectors

\item {} 
Veritas Integrated Connectors

\end{itemize}

\sphinxstylestrong{To add credentials for connectors}
\begin{enumerate}
\item {} 
In the VCC, click \sphinxstylestrong{Credentials}.

\item {} 
On the \sphinxstylestrong{Item List} pane, do the following:
\begin{itemize}
\item {} 
For cloud connectors, click \sphinxstylestrong{New} \sphinxstylestrong{\textgreater{}} \sphinxstylestrong{Cloud Connectors}.

\item {} 
For on-premises connectors, click \sphinxstylestrong{New} \sphinxstylestrong{\textgreater{}} \sphinxstylestrong{On-premises Connectors}.

\end{itemize}

\item {} 
Select the connector for which you want to add credentials.
The \sphinxstylestrong{New credential for \textless{}*name of connector*\textgreater{}} panel opens. Enter the details as described in the tables below.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Azure_creds}.png}
\end{figure}

\end{enumerate}

You can also add, edit, or delete the stored credentials.


\bigskip\hrule\bigskip



\section{Configuring On-cloud content sources}
\label{\detokenize{mcdmp_app_ug:configuring-on-cloud-content-sources}}

\subsection{Configuring data collection from Microsoft Azure}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-microsoft-azure}}
Microsoft Azure Storage provides two authentication mechanisms; Storage Account Keys (SAK) or Shared Access Signatures (SAS). SAKs provide full unrestricted access to a Storage Account and should be kept private and secure at all times; they should also be regularly rotated in line with your Organization’s Information Security Policy. SAS tokens provide restricted access to a Storage Account, allowing read and list only permissions to be set; they are derived using a SAK. SAS tokens are designed to expire, meaning a third party using the SAS token must regularly request a new one.


\subsubsection{Prerequisites for configuring data collection from Microsoft Azure Storage}
\label{\detokenize{mcdmp_app_ug:prerequisites-azure}}\label{\detokenize{mcdmp_app_ug:prerequisites-for-configuring-data-collection-from-microsoft-azure-storage}}
To provide access to discover and scan items stored within Azure Storage, while also keeping your data secure, Veritas leverages Azure Active Directory, Automation Accounts, and Key Vault alongside SAKs and SAS tokens. This mechanism allows a centralized Role-based access control (RBAC) system to control the production of SAS tokens securely in your Azure environment, and allows access to these tokens to VCC through an Azure Active Directory service account.

Microsoft Azure Active Directory provides a centralized user and service account repository with RBAC to control access to Azure resources. Here you can create a service account for Veritas 360 Data Analysis to use to unlock dark data stored within Azure.

Microsoft Automation Accounts provide the ability to automate tasks within your Organization’s Azure environment using PowerShell. Using an automation account you can set up scripts to rotate your SAKs and to generate SAS tokens which will get stored in Azure Key Vault. Microsoft Azure Key Vault can securely store your cryptographic private keys and secrets, such as shared keys and passwords. You can grant privileges to Azure

Active Directory users or services to access the key vault, perform encryption or decryption operations, or read and write secrets.

Veritas provides an example functioning PowerShell script to generate and store SAS tokens into Key Vault. This script that can be installed, modified, and configured according to your requirements.

Before you can configure VCC to start collecting metadata from the Microsoft Azure Storage, you must complete the following tasks within your Azure environment.
\begin{itemize}
\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:create-information-map-account}]{\sphinxcrossref{\DUrole{std,std-ref}{Create an 360 Data Analysis application account}}}}.
Create a Service Principal for the Azure connector in the Azure Active Directory, and provide the Service Principal the Reader role on the storage accounts that you want to monitor.

\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:set-up-key-vault}]{\sphinxcrossref{\DUrole{std,std-ref}{Set up Key Vault with automation}}}}

\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:install-automation-script}]{\sphinxcrossref{\DUrole{std,std-ref}{Install the automation script}}}}.
Create a Service Principal for Azure-Information-Map which is set up to periodically run a PowerShell script (that is provided by Veritas). The script generates SAS tokens and places them in a Key Vault.

\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:record-azure-account-details}]{\sphinxcrossref{\DUrole{std,std-ref}{Record the Azure account details}}}}

\end{itemize}


\paragraph{Create an 360 Data Analysis application account}
\label{\detokenize{mcdmp_app_ug:create-information-map-account}}\label{\detokenize{mcdmp_app_ug:create-an-360-data-analysis-application-account}}
\sphinxstylestrong{Steps to create an application account}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Step
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
How to?
\unskip}\relax \\
\hline
1. Create or reuse an Application
Principal) for the automation
account in Azure Active
Directory. For future reference,
this will be called the
Azure-Information-Map Service
Principal.
&\begin{enumerate}
\item {} 
Log on to the Azure portal \sphinxurl{https://portal.azure.com}.

\item {} 
In the left navigation panel, click \sphinxstylestrong{Azure Active}
\sphinxstylestrong{Directory}.

\item {} 
From the \sphinxstylestrong{Azure Active Directory} pane, click
\sphinxstylestrong{App registrations}.

\item {} 
In the \sphinxstylestrong{App registrations} pane, click
\sphinxstylestrong{New application registration}.

\item {} 
On the \sphinxstylestrong{Create} pane; enter an appropriate name,
select Web app / API as the Application type, and
is the name you choose.

\item {} 
Click \sphinxstylestrong{Create}.

\end{enumerate}
\\
\hline
2. Provide the Azure-Information-Map
Service Principal with Reader role
on the Storage Account(s) you wish to
provide 360 Data Analysis
access to.
&1. In the left navigation panel of the Azure portal,
click \sphinxstylestrong{Storage Accounts}.

2. From the \sphinxstylestrong{Storage accounts} pane, repeat the
following for all storage accounts you want
360 Data Analysis to scan.
\begin{itemize}
\item {} 
Select the storage account

\item {} 
Select \sphinxstylestrong{Access control (IAM)} for your Storage
account.

\item {} 
From the \sphinxstylestrong{Access control (IAM)} pane, click
\sphinxstylestrong{Add}.

\item {} 
In the \sphinxstylestrong{Add permissions} pane, select \sphinxstylestrong{Reader}
as the role, and then search for and select the name
of the Azure-Information-Map Service Principal
you created in \sphinxstylestrong{Step 1}.

\end{itemize}
\\
\hline
3.Provide the Azure Information
Map Service Principal with
Secret Management Rights  on the Key
Vault(s) being used.
&1. In the left navigation panel of the Azure portal
click \sphinxstylestrong{Key Vaults}.

2. From the \sphinxstylestrong{Key Vaults} pane, select the Key vault
you want to use to store the read-only SAS tokens for
360 Data Analysis.

3. In your \sphinxstylestrong{Key Vault} pane, select
\sphinxstylestrong{Access policies}.

4. In the \sphinxstylestrong{Add access policy} pane, select the
following:
\begin{itemize}
\item {} 
\sphinxstylestrong{Secret Management} as the \sphinxstylestrong{Configure from}
\sphinxstylestrong{template} option.

\item {} 
Azure-Information-MapService Principal as the
principal.

\item {} 
Under \sphinxstylestrong{Secret Permissions} ensure that only
\sphinxstylestrong{Get} and \sphinxstylestrong{List} are selected.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Click \sphinxstylestrong{OK}.

\end{enumerate}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\paragraph{Set up Key Vault with automation}
\label{\detokenize{mcdmp_app_ug:set-up-key-vault-with-automation}}\label{\detokenize{mcdmp_app_ug:set-up-key-vault}}
\sphinxstylestrong{Steps to set up the Azure key vault}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Step
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
How to?
\unskip}\relax \\
\hline
1. Create or reuse an Application
Registration (or Service Principal)
for the automation account in Azure
Active Directory. For future
reference, this will be called
the Azure-Automation Service
Principal.
&\begin{enumerate}
\item {} 
Log on to the Azure portal - \sphinxurl{https://portal.azure.com}.

\item {} 
In the left navigation panel, click \sphinxstylestrong{Azure Active}
\sphinxstylestrong{Directory}.

\item {} 
From the Azure Active Directory pane, click \sphinxstylestrong{App}
\sphinxstylestrong{registrations}.

\item {} 
In the \sphinxstylestrong{App registrations} pane, click \sphinxstylestrong{New}
\sphinxstylestrong{application registration}.

\item {} 
On the \sphinxstylestrong{Create} pane; enter an appropriate name,
select Web app /API as the Application type, and enter
a sign-on URL of \sphinxurl{https://{[}name}{]} where {[}name{]} is the
name you choose.

\item {} 
Click \sphinxstylestrong{Create}.

\item {} 
From the \sphinxstylestrong{App registrations} pane, record the
Application ID of the Azure-Automation Service
Principal you just created. This value will be input
into the  Automation as the
AzureSASKeyGeneratorDaemon-ClientId variable.

\item {} 
From the App registrations pane, select the
Azure-Automation Service Principal you just created.

\item {} 
From the resulting right-hand pane, select \sphinxstylestrong{Keys}.

\item {} 
Enter a description and select an appropriate expiry
date that confirms to your Information Security
Policy, and click \sphinxstylestrong{Save}.

\item {} 
Record the displayed value, which is the value of
the Application Secret. This value will be input
into the Automation as the
AzureSASKeyGeneratorDaemon-ClientSecret variable.

\end{enumerate}

Note that the value of the Application Secret is not
displayed again. Ensure that it is recorded before
continuing.
\\
\hline
2.Create or use an existing Key Vault.
Veritas recommends using a new Key
Vault to keep 360 Data Analysis access
to SAS Tokens isolated from other keys
and secrets.

\sphinxstylestrong{Note:} You can use multiple Key
vaults to segregate different business
functions (for example Scanning
Pre-production Storage Accounts, and
Scanning Production Storage Accounts).
&\begin{enumerate}
\item {} 
In the left navigation panel of the Azure portal,
click \sphinxstylestrong{Key Vaults}.

\item {} 
From the \sphinxstylestrong{Key Vaults} pane, click \sphinxstylestrong{Add}.

\item {} 
In the \sphinxstylestrong{Create} \sphinxstylestrong{key} \sphinxstylestrong{vault} pane, input a
name, select a subscription, create or use an existing
resource group and select a location for the Key Vault

\item {} 
Click \sphinxstylestrong{Create}.

\end{enumerate}
\\
\hline
3.Provide the Azure-Automation Service
Principal with Secret Management Rights
on the Key Vault(s) being used.
&\begin{enumerate}
\item {} 
In the left navigation panel of the Azure portal,
click \sphinxstylestrong{Key Vaults}.

\item {} 
From the \sphinxstylestrong{Key Vaults} pane, select the Key vault you
want to use to store the read-only SAS tokens for
360 Data Analysis.

\item {} 
Select \sphinxstylestrong{Access control (IAM)} for your Key vault.

\item {} 
From the \sphinxstylestrong{Access control (IAM)} pane, click
\sphinxstylestrong{Add}.

\item {} 
In the \sphinxstylestrong{Add permissions} pane, select \sphinxstylestrong{Contributor}
as the role; Search for and select the name of the
Azure-Automation Service Principal you created in Step 1

\item {} 
Click \sphinxstylestrong{Save}.

\item {} 
In your \sphinxstylestrong{Key Vault} pane, select \sphinxstylestrong{Access}
\sphinxstylestrong{policies}.

\item {} 
In the \sphinxstylestrong{Add access policy} pane, select the
following:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxstylestrong{Secret Management} as the \sphinxstylestrong{Configure from}
\sphinxstylestrong{template} option.

\item {} 
Azure-Automation Service Principal as the principal.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{8}
\item {} 
Click \sphinxstylestrong{OK}.

\end{enumerate}
\\
\hline
4.Provide the Azure-Automation Service
Principal with Storage Account Key
Operator Service role and Reader
permission on the Storage account(s)
that you want to provide Information
Map access to.
&\begin{enumerate}
\item {} 
In the left navigation panel of the Azure portal,
click \sphinxstylestrong{Storage accounts}.

\item {} 
From the \sphinxstylestrong{Storage accounts} pane, repeat the
following for all storage accounts you want Information
Map to scan:

\end{enumerate}
\begin{itemize}
\item {} 
Select the Storage account.

\item {} 
Select Access control (IAM) for your Storage account.

\item {} 
In the \sphinxstylestrong{Add} \sphinxstylestrong{permissions} pane, select \sphinxstylestrong{Storage}
\sphinxstylestrong{Account} \sphinxstylestrong{Key} \sphinxstylestrong{Operator} Service** as the role;
Search for and select the name of the Azure-Automation
Service Principal you created in Step 1.

\end{itemize}

Once this role has been added repeat this process
but instead select the Reader role.
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Save}.

\end{itemize}
\\
\hline
5. Create or use an existing Automation
account to run a PowerShell script that
will generate SAS Tokens periodically
&\begin{enumerate}
\item {} 
In the left navigation panel of the Azure portal,
click \sphinxstylestrong{More} \sphinxstylestrong{services}, and search for
\sphinxstylestrong{Automation} \sphinxstylestrong{Accounts}.

\item {} 
In the \sphinxstylestrong{Automation} Accounts** pane click \sphinxstylestrong{Add}.

\item {} 
In the \sphinxstylestrong{Add} \sphinxstylestrong{Automation} \sphinxstylestrong{Account} pane,
input a name, select a subscription, create
or use an existing resource group, and
select a location for the Automation account.

\item {} 
Click \sphinxstylestrong{Create}.

\end{enumerate}
\\
\hline
6. Create new Variable Type Assets to
hold the parameters for running the
automation.
&\begin{enumerate}
\item {} 
In the left navigation panel of the Azure portal,
click \sphinxstylestrong{More} \sphinxstylestrong{services} and search for
\sphinxstylestrong{Automation} \sphinxstylestrong{Accounts}.

\item {} 
In the \sphinxstylestrong{Automation} \sphinxstylestrong{Accounts} pane, select
the automation account that you want to use.

\item {} 
In your automation account pane, select \sphinxstylestrong{Variables}.

\item {} 
Click \sphinxstylestrong{Add} to create the variables.

\end{enumerate}
\begin{itemize}
\item {} 
DirectoryDomainName- The domain name of the directory.
To obtain the domain name of the directory, mouse over
the top right hand button in the Azure Portal to display
a tool tip which contains an entry for the Directory
Domain name.
Type: String
Encrypted: No

\item {} 
SubscriptionId - The ID of the Azure subscription.
Type: String
Encrypted: No

\item {} 
AzureSASKeyGeneratorDaemon-ClientId- The client ID of
the Azure-Automation Service Principal.
Type: String
Encrypted: No

\item {} 
AzureSASKeyGeneratorDaemon-ClientSecret-The client
secret of the Azure-Automation Service Principal.
Type: String
Encrypted: Yes

\item {} 
AzureSASKeyGeneratorDaemonStorageAccount
-KeyVaultMapping
The variable holds the information about the storage
accounts for which the SAS tokens should be
regenerated and stored in what key vaults.
Type: String
Encrypted: No
The format of the string is as follows:
\sphinxcode{\textless{}*storage\_account\_name*\textgreater{},\textless{}*storage\_account\_name*\textgreater{}:
\textless{}*key\_vault\_name*\textgreater{};\textless{}*storage\_account\_name*\textgreater{},
\textless{}*storage\_account\_name*\textgreater{} \textless{}*key\_vault\_name*\textgreater{}}

\end{itemize}
\begin{description}
\item[{For example,}] \leavevmode
\sphinxcode{prodservice,prodservicefiles,prodservicelogs:}
\sphinxcode{PROD-Service-SASTokens;prodinfrastorage:
Infra-SASTokens}

\end{description}

The string tells the automation script job instance
(when running) to do the following:
\begin{itemize}
\item {} 
Generate SAS tokens for prodservice, prodservicefiles
and prodservicelogs storage accounts and store them in
the PROD-Service-SASTokens key vault.

\item {} 
Generate SAS Tokens for prodinfrastorage storage and
it in the Infra-SASTokens key vault.account.

\end{itemize}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\paragraph{Install the automation script}
\label{\detokenize{mcdmp_app_ug:install-automation-script}}\label{\detokenize{mcdmp_app_ug:install-the-automation-script}}
\sphinxstylestrong{Steps to install the automation script}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Step
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
How to?
\unskip}\relax \\
\hline\begin{enumerate}
\item {} 
Create a Runbook

\end{enumerate}
&\begin{enumerate}
\item {} 
Log on to the Azure portal -
\sphinxurl{https://portal.azure.com}.

\item {} 
In the left navigation panel, click \sphinxstylestrong{More services}
and search for \sphinxstylestrong{Automation Accounts}.

\item {} 
In the \sphinxstylestrong{Automation Accounts} pane, select the
Automation account you have previously configured.

\item {} 
In your \sphinxstylestrong{Automation Account} pane, select
\sphinxstylestrong{Runbooks}.

\item {} 
Click \sphinxstylestrong{Add a Runbook}.

\item {} 
In the \sphinxstylestrong{Add Runbook} pane, select \sphinxstylestrong{Create a new
Runbook}.

\item {} 
In the \sphinxstylestrong{Create Runbook} pane, input an appropriate
name for the Runbook, for example,
AzureSASTokenGenerator, and select \sphinxstylestrong{PowerShell
Workflow Runbook} as the \sphinxstylestrong{Runbook type}.

\item {} 
Click \sphinxstylestrong{Create}.
An editor is displayed.

\end{enumerate}
\\
\hline
2. Configure the SAS token
generation script.
&\begin{enumerate}
\item {} 
In the editor, paste the provided example script
file.

Note that the script is provided by Veritas and is
distributed freely and can be modified appropriately.

It is t is designed to function as an initial way to
populate Key Vault with SAS tokens. It can be freely
modified, the headers should be kept intact.

\item {} 
If you want to test the script, select \sphinxstylestrong{Test Pane}
from the menu bar.

\item {} 
Click \sphinxstylestrong{Save}, and then click \sphinxstylestrong{Publish} to
make it operational.

\end{enumerate}
\\
\hline
3. Create or use an existing schedule
to run the script periodically
(for example, Every hour).
&\begin{enumerate}
\item {} 
On the Azure portal, in the left navigation panel,
click \sphinxstylestrong{More services} and search for \sphinxstylestrong{Automation
Accounts}.

\item {} 
In the Automation Accounts pane, select the
Automation account you have previously configured.

\item {} 
In the \sphinxstylestrong{Automation Accounts} pane, select
\sphinxstylestrong{Runbooks}.

\item {} 
In the \sphinxstylestrong{Runbooks} pane, select the Runbook
previously created.

\item {} 
In your \sphinxstylestrong{Runbook} pane, click \sphinxstylestrong{Schedules}.

\item {} 
Click \sphinxstylestrong{Add a Schedule}.

\item {} 
In the resulting pane, select \sphinxstylestrong{Link a Schedule to
your Runbook}.

\end{enumerate}
\begin{quote}
\begin{quote}

Select an existing schedule or if you are creating
new schedule, do the following:
\end{quote}
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Create a new schedule}.

\item {} 
Enter an appropriate name and interval that suits
your organization’s Information Security policy.

\item {} 
Click Create.

\end{itemize}
\end{quote}
\begin{description}
\item[{8.After you configure the schedule, select}] \leavevmode\begin{quote}

\sphinxstylestrong{Configure parameters and runsettings} to set the
duration of the validity of SAS tokens (otherwise it
defaults to 525600 seconds which is 1 year.
\end{quote}
\begin{itemize}
\item {} 
Enter a value in seconds for \sphinxstylestrong{TOKENDURATION} in
accordance with your organization’s Information
Security Policy.

\item {} 
Click \sphinxstylestrong{OK}.

\end{itemize}

\end{description}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\paragraph{Record the Azure account details}
\label{\detokenize{mcdmp_app_ug:record-azure-account-details}}\label{\detokenize{mcdmp_app_ug:record-the-azure-account-details}}
Record the information about the Microsoft Azure account as described in the below table that is required when configuring your Azure Storage account in VCC.

\sphinxstylestrong{Data required for configuring Azure Storage account in VCC}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Step
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
How to?
\unskip}\relax \\
\hline
Tenant ID, which is the ID of the
Azure Active Directory in which you
created the application.
&\begin{enumerate}
\item {} 
On the Azure portal, in the left navigation panel,
click \sphinxstylestrong{Azure Active Directory}.

\item {} 
Select \sphinxstylestrong{Properties} for your Azure AD tenant.

\item {} 
From the \sphinxstylestrong{Properties} pane, copy the \sphinxstylestrong{Directory}
\sphinxstylestrong{ID}. This is the value of the Tenant ID.

\end{enumerate}
\\
\hline
Client ID which is the ID of the Azure
Active Directory Application you
created. The application ID is passed
along with the authentication request
when 360 Data Analysis logs in
to Azure
&\begin{enumerate}
\item {} 
On the Azure portal, in the left navigation panel,
click \sphinxstylestrong{Azure Active Directory}.

\item {} 
Select \sphinxstylestrong{App registrations} for your Azure
Directory Instance.

\item {} 
From the \sphinxstylestrong{App registrations} pane, record the
Application ID of the Azure-Information-Map Service
Principal you created previously. This is the value
of the Client ID.

\end{enumerate}
\\
\hline
Create and Record the Client Secret,
which is the login secret of the Azure
Active Directory Application account
which you create. The Application
secret is passed along with the
authentication request when
360 Data Analysis logs in to Azure.
&\begin{enumerate}
\item {} 
On the Azure portal, in the left navigation panel,
click \sphinxstylestrong{Azure Active Directory}.

\item {} 
Select \sphinxstylestrong{App registrations} for your Azure Directory
Instance.

\item {} 
From the App registrations pane, record the
Application ID of the Azure-Information-Map Service
Principal you created previously.

\item {} 
From the resulting right-hand pane, select \sphinxstylestrong{Keys}.

\item {} 
Enter a description and select an appropriate expiry
that confirms to your Information Security Policy,
and click \sphinxstylestrong{Save}.

\item {} 
Record the value displayed. This is the value of the
Application Secret. This is the value of the Client
Secret.

\end{enumerate}

\begin{sphinxadmonition}{note}{Note:}
The client secret is not displayed again.
Record it before continuing.
\end{sphinxadmonition}
\\
\hline
Key Vault URL - URL of the Azure Key
Vault in which you are storing read
only SAS Tokens. The key vault URL is
used to retrieve the SAS tokens
before authenticating 360 Data Analysis
to Azure Storage.
&\begin{enumerate}
\item {} 
On the Azure portal, in the left navigation
panel, click \sphinxstylestrong{Key Vaults}.

\item {} 
From the \sphinxstylestrong{Key Vaults} pane, select the key vault
you are using to store read-only SAS tokens for
360 Data Analysis.

\item {} 
Select \sphinxstylestrong{Properties} for your key vault.

\item {} 
From the \sphinxstylestrong{Properties} pane, record the DNS name
This is the value of the key vault URL.

\end{enumerate}
\\
\hline
Azure subscription ID. The subscription
ID is a GUID that uniquely identifies
subscription to use Azure services.
&\begin{enumerate}
\item {} 
Log on to the Azure portal -\sphinxurl{https://portal.azure.com}.

\item {} 
In the left navigation panel, click \sphinxstylestrong{Subscriptions}

\item {} 
From the \sphinxstylestrong{Subscriptions} pane, record the
subscription ID of the subscription you choose to be
scanned.

\end{enumerate}
\\
\hline
Azure environment - Identify your
Azure environment. This ensures that
the correct URLs are used when access
Azure resources.
Information about the Azure environment
default to Azure. The available options
are; \sphinxstyleemphasis{Azure, Azure China, Azure
Germany} or \sphinxstyleemphasis{Azure US Government}.
&
You can discover your Azure environment using the
the DNS suffix of the key vault URL:
*  .vault.azure.cn \textendash{} Azure China
*  .vault.usgovcloudapi.net \textendash{} Azure US Government
*  .vault.azure.net \textendash{} Azure
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Configuring credentials for Microsoft Azure}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-for-microsoft-azure}}
Ensure that you complete the prerequisite steps before you add the credentials for Microsoft Azure.

{\hyperref[\detokenize{mcdmp_app_ug:prerequisites-azure}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites for configuring data collection from Microsoft Azure Storage}}}}

\sphinxstylestrong{New credential for Microsoft Azure}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
Enter a logical name for the credential. It can be your tenant ID or
the Azure Active Directory ID. The name you specify here helps you
select the relevant credential when configuring  the Microsoft
Azure connection.
\\
\hline
Description
&
Enter a description for the credential. This field is optional.
\\
\hline
Tenant ID
&
Enter the ID of the Azure Active Directory in which you created the
application.
The details of the tenant ID, Client Id, secret key, and the key vault
URI are created a recorded as a part of completing the prerequisite
steps.
\\
\hline
Client ID
&
Enter the ID of the Azure Active Directory Application you created.
\\
\hline
Secret Key
&
Enter the login secret of the Azure Active Directory Application
account which you create.
\\
\hline
Key vault URI
&
Enter URL of the Azure Key Vault in which you have stored the read-only
SAS Tokens. The key vault URL is used to retrieve the SAS tokens before
authenticating 360 Data Analysis to Azure Storage.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Adding a Microsoft Azure connection}
\label{\detokenize{mcdmp_app_ug:adding-a-microsoft-azure-connection}}
Before you set-up a connection to allow applications to discover the Microsoft Azure storage accounts, you must complete all the prerequisites to authorize the access.

{\hyperref[\detokenize{mcdmp_app_ug:prerequisites-azure}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites for configuring data collection from Microsoft Azure Storage}}}}

\sphinxstylestrong{Adding a connection for a Microsoft storage account}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your Microsoft Azure account.
The name that you enter in this field represents a content
source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the Azure
storage account.
\\
\hline
Subscription ID
&
Enter the Azure subscription ID, which is a GUID that uniquely
identifies your subscription to use Azure services.
You must record the subscription ID as a part of the prerequisite
steps for configuring Azure from VCC.
\\
\hline
Azure environment
&
Select your Azure environment. This step is optional. By default, Azure
is selected.
The available options are; Azure, Azure China, Azure
Germany, or Azure US Government.
\\
\hline
Credential
&
From the drop-down, select the respective saved credential.
\\
\hline
Group
&
Select the group of the connector node which is responsible for
scanning the Azure storage account.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Microsoft SharePoint}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-microsoft-sharepoint}}
Microsoft SharePoint Online is a cloud-hosted connector that connects to a your Office 365 account to discover the site collections, sites and sub-sites, document libraries, files, and folders. It scans each site collection to collect summary metadata and uploads the metadata to 360 Data Analysis for analysis and geographical
visualization.


\subsubsection{Adding a SharePoint Online connection}
\label{\detokenize{mcdmp_app_ug:adding-a-sharepoint-online-connection}}
Note that you do not need to add credentials in VCC for the Microsoft SharePoint Online data store. You will be redirected to the Office 365 login page for authorization at the time of adding the connection. SharePoint Online uses the Open Authorization 2 (OAuth2) protocol to permit access to a third-party application, which is done
using a Microsoft-registered application.

Add a connection of type Microsoft SharePoint Online in Veritas Connection Center to discover SharePoint site collections.

\sphinxstylestrong{Adding a SharePoint Online account in VCC}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your SharePoint Online account. The name that you enter in
this field represents a content source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
SharePoint Online account.
\\
\hline
Authorize on
on Office365.com
&Do the following:
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Authorize on Office365.com} to display the Microsoft
SharePoint Online authorization page.

\end{itemize}
\begin{quote}

To scan the SharePoint Online account and the underlying site
collections the Connector agent uses one of following default apps
depending on your location - Veritas 360 Data Analysis (EMEA),
Veritas 360 Data Analysis (NAM).

Login using Microsoft global administrator credentials that Connector
will use to gain access to the SharePoint Online account, and click
\sphinxstylestrong{Sign In}.

You will return to the VCC console once the authorization
is complete
\end{quote}
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Save} to add the connection.

\end{itemize}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Enterprise Box}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-enterprise-box}}
For cloud data stores, such as Box and Microsoft OneDrive, you do not need to add credentials in VCC. The authentication for these data stores is controlled using
the OAuth 2.0 workflow for each connector. For these data stores, you are redirected to the respective tenant accounts for authorization, where you must perform an
interactive registration to acquire the authentication tokens.


\subsubsection{Adding a Box for Enterprise connection}
\label{\detokenize{mcdmp_app_ug:adding-a-box-for-enterprise-connection}}
Box uses the Open Authorization 2 (OAuth2) protocol to permit access to a third-party application. Veritas Connection Center (VCC) uses Box Enterprise administrator credentials to scan a Box Administrator account. The Administrator credentials authorize VCC to access the Box Enterprise account. The authorization received from Box is encrypted and saved in the VCC configuration.

The credentials are used by the Veritas cloud agent to impersonate a user account to query Box for metadata.

Note that you do not need to add credentials in VCC for the Box data store. You will be redirected to the respective tenant accounts for authorization at the time of adding the connection.

Enter the following details to add a connection for Box for Enterprise:

\sphinxstylestrong{Adding a Box for Enterprise connection}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your Box account. The name that you enter in this
field represents a content source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
Box account.
\\
\hline
Group
&
Select the group of the connector node which is responsible for
scanning the Box account.
\\
\hline
Authorize on Box.com
&
Do the following:
\begin{enumerate}
\item {} 
Click \sphinxstylestrong{Authorize on Box.com} to display the Box authorization page.

\item {} 
Specify the Box administrator credentials that Connector Framework
uses to gain access to the Box account, and click \sphinxstylestrong{Authorize}.

\item {} 
Click \sphinxstylestrong{Grant Access to Box}. This step creates an authorization
token that is stored as a named credential in the VCC configuration.
A third-party application, such as, 360 Data Analysis can now access
the user, folder and file metadata.

\end{enumerate}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Google Cloud Storage}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-google-cloud-storage}}
The Google Cloud Storage platform lets you store your data into regional and multi-regional buckets (content sources) within your Google Cloud Storage projects. The Google Cloud Storage connector is a cloud-hosted connector. It connects to your Google Cloud Storage project, discovers and records the data locations and buckets, scans each bucket to collect metadata on the items that are discovered, and uploads the metadata to 360 Data Analysis for analysis and geographical visualization.

Deployment of the Google Cloud Storage connector is handled by the Veritas Client Services team.


\subsubsection{Prerequisites for configuring data collection from Google Cloud Storage}
\label{\detokenize{mcdmp_app_ug:prereq-google-cloud-storage}}\label{\detokenize{mcdmp_app_ug:prerequisites-for-configuring-data-collection-from-google-cloud-storage}}
Before you can create a connection for Google Cloud Storage from the VCC, you must complete the following prerequisite steps:
\begin{enumerate}
\item {} 
Create a service account for your project on the Google Cloud Storage platform. This service account is used by 360 Data Analysis to gain access to the Google buckets that you want to discover. See {\hyperref[\detokenize{mcdmp_app_ug:create-service-account}]{\sphinxcrossref{\DUrole{std,std-ref}{Creating a service account}}}}.

\item {} 
Generate a key for the service account. Creating a key for the service account

\end{enumerate}


\paragraph{\sphinxstylestrong{Creating a service account}}
\label{\detokenize{mcdmp_app_ug:creating-a-service-account}}\label{\detokenize{mcdmp_app_ug:create-service-account}}
\sphinxstylestrong{To create a service account}
\begin{enumerate}
\item {} 
Log on to \sphinxurl{https://console.cloud.google.com}.

\item {} 
Select the project for which you want to create a service account. For example, InfoMapTest. This is the project that contains the Google buckets that you want to discover.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{GCP}.png}
\end{figure}

\item {} 
Select \sphinxstylestrong{IAM \& Admin \textgreater{} Service Accounts}.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{SelectSA}.png}
\end{figure}

\item {} 
Select \sphinxstylestrong{Add a new service account}.

\item {} 
On the \sphinxstylestrong{Create Service account} page, enter a service account name.

\item {} 
Set the role for the service account to \sphinxstylestrong{Storage \textgreater{} Storage Admin}.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Create_SA}.png}
\end{figure}

\item {} 
Click \sphinxstylestrong{Create}.

\end{enumerate}


\paragraph{\sphinxstylestrong{Creating a key for the service account}}
\label{\detokenize{mcdmp_app_ug:creating-a-key-for-the-service-account}}
\sphinxstylestrong{To create a service account key}
\begin{enumerate}
\item {} 
In the Google Cloud Platform, select the relevant service account from the list of configured service accounts. Click \sphinxstylestrong{Create key}.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{Create_service_ac}.png}
\end{figure}

\item {} 
On the \sphinxstylestrong{Create private key} page for \textless{}\sphinxstyleemphasis{Name of service account}\textgreater{}, select JSON, and click \sphinxstylestrong{Create}. This step downloads the key file to your computer. Make a note of the location where the key file is saved. The contents of the key are required when you configure the credentials to your Google Cloud Storage account.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics[scale=1.0]{{create_pvt_key}.png}
\end{figure}

\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:config-creds-google-cloud-storage}]{\sphinxcrossref{\DUrole{std,std-ref}{Configuring credentials for Google Cloud Storage}}}}

\item {} 
{\hyperref[\detokenize{mcdmp_app_ug:adding-google-cloud-storage}]{\sphinxcrossref{\DUrole{std,std-ref}{Adding a Google Cloud Storage connection}}}}

\end{enumerate}
\begin{quote}
\phantomsection\label{\detokenize{mcdmp_app_ug:config-creds-google-cloud-storage}}\end{quote}


\subsubsection{Configuring credentials for Google Cloud Storage}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-for-google-cloud-storage}}\label{\detokenize{mcdmp_app_ug:config-creds-google-cloud-storage}}
The following section describes how you can configure the credentials required for Google Cloud Storage data store.

Ensure that you complete the prerequisite steps before you add the credentials for Google Cloud Storage.

Enter the following details to add the cloud connection:
\begin{quote}

\sphinxstylestrong{New credential for Google Cloud Storage}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
Enter a logical name for the credential. The name you specify here helps
you select the relevant credential when configuring the Google Cloud
Storage connection.
\\
\hline
Description
&
Enter a description for the credential. This field is optional.
\\
\hline
Service Account Key
&
Enter the service account key in the JSON format.
Copy and paste the contents of the key that is created and
downloaded in the preceding step.
\\
\hline
Project ID
&
This field is auto-populated if the Project ID is included in the
service account key JSON.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Adding a Google Cloud Storage connection}
\label{\detokenize{mcdmp_app_ug:adding-a-google-cloud-storage-connection}}\label{\detokenize{mcdmp_app_ug:adding-google-cloud-storage}}
Before you set-up a connection to allow applications, such as 360 Data Analysis to discover the Google Cloud Storage buckets and objects, you must configure the required permission and roles to authorize the access.

\begin{sphinxadmonition}{note}{Note:}
Currently, the Google Cloud Storage connector only supports one Google cloud Storage project per VCC connection. Service accounts that have access to multiple projects are not supported.
\end{sphinxadmonition}

Enter the following details to add a connection for Google Cloud Storage:
\begin{quote}

\sphinxstylestrong{Adding a connection for a Google Cloud Storage account}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your Google Cloud Storage account.
The name that you enter in this field represents a content source in
360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the Google
Cloud Storage account.
\\
\hline
Credential
&
From the drop-down, select the respective saved credential.
\\
\hline
Group
&
Select the group of the connector node which is responsible for
scanning the Google Cloud Storage account.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Microsoft Exchange Online}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-microsoft-exchange-online}}
Exchange Online is a cloud-hosted connector that connects to a your Office 365 account or your Microsoft Exchange Online account to discover the Exchange mailboxes of users. It scans each mailbox to collect summary metadata, such as email addresses of all users for a tenant account, and uploads the metadata to
Analysis for analysis and geographical visualization.


\subsubsection{Configuring credentials for Microsoft Exchange Online}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-for-microsoft-exchange-online}}
The Exchange Online Connector requires credentials with appropriate roles and permissions to be able to connect to Exchange and discover the account metadata.

\sphinxstylestrong{New credential for Microsoft Exchange Online}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
Enter a logical name for the credential. The name you specify here
helps you select the relevant credential when configuring the Exchange
Online connection.
\\
\hline
Description
&
Enter a description for the credential. This field is optional.
\\
\hline
Username
&
Enter the user name of the service account with the
ApplicationImpersonation role to be able to impersonate other users and
access their mailbox details.
\\
\hline
Password/
Confirm Password
&
Enter the relevant password for the service account.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Adding a connection for Microsoft Exchange Online}
\label{\detokenize{mcdmp_app_ug:adding-a-connection-for-microsoft-exchange-online}}
Add a connection of type Microsoft Exchange Online in Veritas Connection Center to discover Exchange Online accounts. Exchange Online uses the Open Authorization 2 (OAuth2) protocol to permit access to a third-party application.

\sphinxstylestrong{Adding a Exchange Online account in VCC}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your Exchange Online account. The name that you enter in
this field represents a content source in Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
Exchange Online account.
\\
\hline
Server
&
Enter the URL for your Exchange Online account.
\\
\hline
Authorize on
OneDrive.com
&Do the following:
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Authorize on Office365.com} to display the Microsoft
Exchange Online authorization page. Enter the credentials of a user
with \sphinxstyleemphasis{ApplicationImpersonation} role, and click \sphinxstylestrong{Sign In}.
You will return to the VCC console once the authorization is complete.

\end{itemize}
\begin{itemize}
\item {} 
Click \sphinxstylestrong{Save} to add the connection.

\end{itemize}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Microsoft OneDrive}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-microsoft-onedrive}}
For cloud data stores, such as Box and Microsoft OneDrive, you do not need to add credentials in VCC. The authentication for these data stores is controlled using
the OAuth 2.0 workflow for each connector. For these data stores, you are redirected to the respective tenant accounts for authorization, where you must perform an
interactive registration to acquire the authentication tokens.


\subsubsection{Adding a Microsoft OneDrive connection}
\label{\detokenize{mcdmp_app_ug:adding-a-microsoft-onedrive-connection}}
Add a connection of type Microsoft OneDrive in Veritas Connection Center to discover OneDrive accounts. OneDrive uses the Open Authorization 2 (OAuth2) protocol to permit access to a third-party application.

Note that you do not need to add credentials in VCC for the Microsoft OneDrive data store. You will be redirected to the respective tenant accounts for authorization at the time of adding the connection.

For every OneDrive account, 360 Data Analysis fetches metadata for files and folders residing on the user drives of a OneDrive account. In the 360 Data Analysis configuration, the OneDrive tenant account (your organization’s OneDrive account) corresponds to a content source and the individual user accounts correspond to containers.

\sphinxstylestrong{Prerequisites for discovering OneDrive user accounts}

The Agent is able to discover only those OneDrive user accounts for which the Access files privilege is enabled in the Office365 Admin Center.

\sphinxstylestrong{To enable Access files permission for user accounts}
\begin{enumerate}
\item {} 
Log in to the Office 365 Admin Center with global administrator credentials.

\item {} 
Navigate to \sphinxstylestrong{Users \textgreater{} Active Users}.

\item {} 
Select the user that you want to discover.

\item {} 
Expand the OneDrive \sphinxstylestrong{Settings} section.

\item {} 
Click \sphinxstylestrong{Access files}.

\item {} 
Repeat steps 3-5 for every user that you want to discover.

\end{enumerate}

\begin{sphinxadmonition}{note}{Note:}
You can also use a script to enable the Access files permission for all configured users at once.
\end{sphinxadmonition}

Enter the following details to add a connection for OneDrive account:

\sphinxstylestrong{Adding a OneDrive account in VCC}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your OneDrive account. The name that you enter in this
field represents a content source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
OneDrive account.
\\
\hline
Group
&
Select the group of the connector node which is responsible for
scanning the OneDrive account.
\\
\hline
Authorize on
OneDrive.com
&
Do the following:
\begin{quote}

1. Click \sphinxstylestrong{Authorize on OneDrive.com} to display the Microsoft OneDrive
authorization page.

2. To scan OneDrive tenant account and the underlying user accounts,
the Connector agent uses the following Veritas 360 Data Analysis (EMEA),
Veritas 360 Data Analysis (NAM) apps depending on your location.

Specify the credentials of a Microsoft global administrator user that
Connector will use to gain access to the OneDrive account and discover
the underlying user accounts.

3. Click \sphinxstylestrong{Sign In}.
You will return to the VCC console once the authorization is complete.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Click \sphinxstylestrong{Save} to add the connection.

\end{enumerate}
\end{quote}
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\section{Configuring On-premises content sources}
\label{\detokenize{mcdmp_app_ug:configuring-on-premises-content-sources}}

\subsection{Configuring data collection from Native File Server}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-native-file-server}}

\subsubsection{Prerequisites for configuring data collection from Native File Server connections}
\label{\detokenize{mcdmp_app_ug:prereq-nfs}}\label{\detokenize{mcdmp_app_ug:prerequisites-for-configuring-data-collection-from-native-file-server-connections}}
For VCC to monitor the Native File Server connections, make sure that the following requirements are completed for different device types:

For the generic requirements that must be accomplished in addition to these prerequisites, see {\hyperref[\detokenize{mcdmp_app_ug:requirements-generic}]{\sphinxcrossref{\DUrole{std,std-ref}{Requirements and prerequisites}}}}.

\sphinxstylestrong{NetApp Standalone}
\begin{itemize}
\item {} 
The DNS lookup and reverse-lookup for host name of the on-premise agent node from the filer must work fine.

\item {} 
The Connector node must have share-level READ permissions on the NetApp server to scan the NFS share that resides on that server.
\begin{itemize}
\item {} 
The Client for NFS service must be installed on all on-premises agent nodes in a service group that are responsible for scanning the native file system content sources.

\item {} 
To enable Connector to discover the NFS shares on the NetApp filer, enable the export of NFS shares on the filer.

\end{itemize}

\end{itemize}

\sphinxstylestrong{NetApp Cluster}
\begin{itemize}
\item {} 
ONTAP version 8.2.1 or higher cluster is configured in accordance with NetApp documentation. Note that Native File Server Connectors support the monitoring of NFS exports on Clustered Data ONTAP (cDOT) versions 8.2.3 and 8.3.1 or higher.

\item {} 
The Connector node must have share-level READ permissions on the NetApp server to scan the NFS share that resides on that server.

\item {} 
The Client for NFS service must be installed on all on-premises agent nodes in a service group that are responsible for scanning the native file system content sources.

\item {} 
To enable Connector to discover the NFS shares on the NetApp Cluster-Mode file server, enable the export of NFS shares on the filer.

\item {} 
Connector Framework should be able to communicate with the CIFS server hosted within the ONTAP cluster and the SVM which has NFS protocol enabled.

\item {} 
Run the following command on each VServer where you want to enable NFS monitoring:

\end{itemize}
\begin{quote}

\sphinxcode{vserver nfs modify -vserver \textless{}vserver name\textgreater{} -v3-ms-dos-client enabled}
\end{quote}

\sphinxstylestrong{EMC Isilon}
\begin{itemize}
\item {} 
Microsoft .Net Framework version 4.5 is installed on the on-premise agent node.

\item {} 
Note the port number from the URL used to access the Isilon OneFS Management Console. This port number is used by Connectors for discovery purposes. The default port is 8080. Ensure that this port is not blocked by the Windows firewall in the on-premise agent node.

\end{itemize}

\sphinxstylestrong{Veritas File System (VxFS)}
\begin{itemize}
\item {} 
Ensure that the on-premise agent node monitoring the VxFS filer has services for NFS enabled as file server roles.

\item {} 
To enable Connector to discover the NFS shares on VxFS filers, enable the export of NFS shares on the filer.

\end{itemize}


\paragraph{Enabling export of NFS shares on a NetApp file server}
\label{\detokenize{mcdmp_app_ug:enabling-export-of-nfs-shares-on-a-netapp-file-server}}
Before you add a NetApp filer to VCC, you must enable the export of NFS shares on the NetApp filer to allow Connector to discover the NFS shares on the filer.

\begin{sphinxadmonition}{note}{Note:}
Connector Framework does not support scanning of NFS shares using a Connector node that is running Windows Server 2012 or Windows Server 2012 R2 edition.
\end{sphinxadmonition}

\sphinxstylestrong{To enable export of NFS shares on the NetApp filer}
\begin{enumerate}
\item {} 
On the NetApp filer web console, select \sphinxstylestrong{NFS \textgreater{} Manage exports}.

\item {} 
On the Export wizard, click \sphinxstylestrong{Add Export} or you can edit the existing exports to modify them.

\item {} 
On the first page of the wizard ensure that you have at least selected read only and root access, other options can also be specified, as required, and click \sphinxstylestrong{Next}.

\item {} 
Define the export path and give read only access to the Connector node, and click \sphinxstylestrong{Next}.

\item {} 
On the Read-Write Access page, enable read-write access for all clients or for specific hosts, as per your need.

\item {} 
Click \sphinxstylestrong{Next}.

\item {} 
On the Root Access page, define root access to the Connector node, and click \sphinxstylestrong{Next}.

\item {} 
On the Security page, accept the default options, and click \sphinxstylestrong{Next}.

\item {} 
On the Summary page, review the configuration and click \sphinxstylestrong{Commit} to save the changes.

\end{enumerate}


\paragraph{\sphinxstylestrong{Enabling export of NFS shares on a NetApp Cluster-Mode file server}}
\label{\detokenize{mcdmp_app_ug:enabling-export-of-nfs-shares-on-a-netapp-cluster-mode-file-server}}
Before you add a NetApp Cluster-Mode file server to VCC, you must identify the volumes and export policies to allow Connector to discover the NFS shares on the cluster.

The Connector requires Read only access to root namespace (/) and super user (ROOT) access to volume namespace to scan NFS exports. If the root namespace and volume are configured with two different export policies, then you must provide access to the Connector node under each export policy rules to enable Connector to scan the NFS shares.

\sphinxstylestrong{To enable export of NFS shares on a NetApp Cluster-Mode file server}
\begin{enumerate}
\item {} 
To verify whether the Connector IP address is specified in the export policy rule, execute the following command.

\end{enumerate}
\begin{quote}

\sphinxcode{export-policy rule show -vserver \textless{}name of vserver\textgreater{} -policyname \textless{}export policy\textgreater{} -clientmatch \textless{}IP of Connector node\textgreater{}}
\end{quote}

The command must be repeated if root namespace is configured to use different export policies.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
The following command will display a denied access message if the IP address of the Connector host is not specified in the export policy rule.

\end{enumerate}
\begin{quote}

\sphinxcode{export-policy check-access -vserver \textless{}vserver name\textgreater{} -volume \textless{}volume name\textgreater{} -client-ip \textless{}IP of Connector\textgreater{} -authentication-method sys -protocol nfs3 -access-type read}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
If the Connector node is not specified in the export policy, modify the export policy rule. Run the following command:

\end{enumerate}
\begin{quote}

\sphinxcode{export-policy rule modify -vserver \textless{}name of vserver\textgreater{} -policyname \textless{}export policy\textgreater{} -clientmatch \textless{}IP of Connector\textgreater{}}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
To validate the access given, run the following command and check read/write access is shown:

\end{enumerate}
\begin{quote}

\sphinxcode{export-policy check-access -vserver \textless{}vserver name\textgreater{} -volume \textless{}volume name\textgreater{} -client-ip \textless{}IP of Connector\textgreater{} -authentication-method sys -protocol nfs3 -access-type read-write}
\end{quote}


\paragraph{\sphinxstylestrong{Enabling export of UNIX/Linux NFS shares on VxFS filers}}
\label{\detokenize{mcdmp_app_ug:enabling-export-of-unix-linux-nfs-shares-on-vxfs-filers}}
These instructions are for Red Hat Enterprise Linux operating system which has standalone Storage Foundation 6.0 installed and a file system created using VxFS. The steps will change depending upon other operating system flavors.

\sphinxstylestrong{To enable export of NFS shares on VxFS filers}
\begin{enumerate}
\item {} 
Login as root on the VxFS filer and open the /etc/exports file.

\item {} 
Specify the name of the share that you would like to monitor.  For example, /demoshare, where the VxFS file system is mounted. Ensure that the device entries are added in /etc/fstab to automatically mount NFS file systems after reboot. Data Insight uses /etc/exports and /etc/fstab for NFS share discovery. Sample entries are shown below:

\end{enumerate}
\begin{description}
\item[{\sphinxcode{root@RHEL5-VxFS \textasciitilde{}{]}\# cat /etc/fstab \textbar{} grep vxfs}}] \leavevmode
\sphinxcode{/dev/vx/dsk/openldapdg/vol01 /openldaphome vxfs defaults,\_netdev 0 0}
\sphinxcode{/dev/vx/dsk/openldapdg/vol02 /data vxfs defaults,\_netdev 0 0}
\sphinxcode{/dev/vx/dsk/openldapdg/vol03 /didata vxfs defaults,\_netdev 0 0}
\sphinxcode{{[}root@RHEL5-VxFS \textasciitilde{}{]}\# cat /etc/exports}
\sphinxcode{/openldaphome 192.168.0.10(ro,sync,no\_root\_squash) 192.168.0.11}
\sphinxcode{(rw,syc) /data/exportshare *(rw,sync,no\_root\_squash)}
\sphinxcode{/didata * (rw,sync,no\_root\_squash)}

\end{description}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Specify the root access and read only access to VCC Collector node. For example,

\end{enumerate}

\sphinxcode{/demoshare \textless{}Collector node IP\textgreater{} (ro,sync,no\_root\_squash)}
\sphinxcode{ro:read only}
\sphinxcode{no\_root\_squash: root access.}

You can specify read +write, \sphinxcode{root\_squash}, \sphinxcode{anonuid}, \sphinxcode{anongid} or other settings, as required.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Run the following command to start the NFS daemon

\end{enumerate}

\sphinxcode{\#service nfs start}


\subsubsection{Configuring credentials for Native File Servers}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-for-native-file-servers}}
When you configure a connection in VCC, the connector accesses the connections on behalf of a user account associated with the connection. The account must have permissions to discover and scan the Native File Servers data store. The connector can effectively use the account only when the corresponding credentials are configured on VCC.

\sphinxstylestrong{Adding credential for Native File Servers connections}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a unique and logical name which
you can identify when configuring a Native File Server data store
in VCC.
The username and password will be associated with the display name.
\\
\hline
Description
&
Enter a brief description for the Native File Server data store. You
can use this description to distinguish between multiple Native File
Servers data stores. This is an optional field.
\\
\hline
Domain
&
The name of the domain to which the user belongs. This is an
optional field.
\\
\hline
Username
&
Enter the username for authentication. The username should belong to
the user who has certain privileges on the Native File Servers
connections.
\\
\hline
Password
&
Enter the password. This field allows lowercase letters, uppercase
letters, numerals, and special characters (@, \#, \&, etc.)..
\\
\hline
Confirm Password
&
Re-enter the password for verification.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\paragraph{Native File Server credentials for data store type}
\label{\detokenize{mcdmp_app_ug:native-file-server-credentials-for-data-store-type}}
\sphinxstylestrong{EMC Celerra or EMC VNX}

For discovery of shares: The credential must belong to the EMC filer Control Station user who has administrative rights including XMLAPI v2 privilege (for example, nasadmin).

For scanning the shares: The credential must belong to the user in the domain of which the EMC filer is a part.

To scan CIFS shares successfully, you must have the share-level READ permission. Additionally, the folder within the share must have the following file system ACLs:
\begin{itemize}
\item {} 
Traverse Folder/Execute File

\item {} 
List Folder/Read Data

\item {} 
Read Attributes

\end{itemize}

\sphinxstylestrong{EMC Isilon}

For discovery of shares: Requires a user account on Isilon to perform automatic discovery of CIFS shares and to list all local groups, group memberships, and local users. The connector can use a non-administrator account for this purpose.

For scanning the shares: Required for scanning of shares from the Isilon cluster. This credential belongs to the user in the domain of which the Isilon is a part.

You must have the share-level READ permission. Additionally, the folder within the share must have the following file system ACLs:
\begin{itemize}
\item {} 
Traverse Folder/Execute File

\item {} 
List Folder/Read Data

\item {} 
Read Attributes

\item {} 
Read Extended Attributes

\item {} 
Read Permissions

\end{itemize}

\sphinxstylestrong{Hitachi NAS}

\begin{sphinxadmonition}{note}{Note:}
The credentials for discovery and scanning can be same.
\end{sphinxadmonition}

For discovery and scan of shares: The credential must belong to a domain user with share-level READ on Hitachi NAS EVS to perform the following tasks:
\begin{itemize}
\item {} 
To discover shares.

\item {} 
To scan the shares for metadata.

\end{itemize}

\sphinxstylestrong{NetApp Cluster}

For discovery of shares: The credential must belong to the NetApp ONTAP cluster root user who is a local user on the ONTAP cluster. Or, this credential must belong to the ONTAP cluster non-administrator user with specific privileges.

This account can be a local account or a domain account. For scanning the shares: When scanning CIFS shares, this credential belongs to the user in the domain of which the NetApp filer is a part. The connector can use a non-administrator account for this purpose.

You must have the share-level READ permission. Additionally, the folder within the share must have the following file system ACLs:
\begin{itemize}
\item {} 
Traverse Folder/Execute File

\item {} 
List Folder/Read Data

\item {} 
Read Attributes

\item {} 
Read Extended Attributes

\item {} 
Read Permissions

\end{itemize}

\sphinxstylestrong{NetApp Standalone}

For discovery of shares: The credential should belong to a user in the domain of which the NetApp filers are a part of.

For scanning the shares: When scanning CIFS shares, this credential belongs to the user in the domain of which the NetApp filer is a part of. Typically, to scan CIFS shares, you must have the share-level READ permission. Additionally, the folder within the share must have the following file system ACLs enabled for the scan credential:
\begin{itemize}
\item {} 
Traverse Folder/Execute File

\item {} 
List Folder/Read Data

\item {} 
Read Attributes

\item {} 
Read Extended Attributes

\item {} 
Read Permissions

\end{itemize}

For scanning NFS shares, the connector needs a UNIX account with at least read and execute permissions on all folders, along with at least Read permission on all files. By default, the connector uses User ID or Group ID 0 to scan NFS shares.

\sphinxstylestrong{Windows File Server (CIFS)}

For discovery of shares: Required for monitoring shares or when configuring a Windows File Server cluster. This credential belongs to a user who has share-level READ permissions on the file server.

For scanning the shares: This credential must belong to a user with necessary share-level permissions on a Windows File Server share.

To be able to scan a Windows File Server share successfully, you must have the share-level READ permission. Additionally, the folder within the share must have the following file system ACLs:
\begin{itemize}
\item {} 
Traverse Folder/Execute File

\item {} 
List Folder/Read Data

\item {} 
Read Attributes

\item {} 
Read Extended Attributes

\item {} 
Read Permissions

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
To enable the connector to successfully scan the shares on a clustered Windows File Server, ensure that the scanning user has domain level permissions of Allow logon locally.
\end{sphinxadmonition}

\sphinxstylestrong{Veritas File System}

For discovery of shares: Required to discover shares on the VxFs filer. This credential belongs to a user on the UNIX server who has share-level READ permissions on the VxFS filer (for example, root). The credential should belong to a root user on the VxFS filer.

To configure a user other than the root user, you must create or use an existing user account, which you can use to add the filer into the connector namespace. To add a local user account under VOM:
\begin{enumerate}
\item {} 
Log on as root on the VxFs filer.

\item {} 
Change directory to /opt/VRTSsfmh/di/web/admin.

\item {} 
Create a .xprtlaccess file, and add the user to that file. For example, add \sphinxhref{mailto:vomuser@unixpwd}{vomuser@unixpwd}:user, where vomuser is the name of the local user account.

\end{enumerate}

For scanning the shares: For scanning NFS shares, the connector needs a domain account or a LocalSystem account with at least read and execute permissions on all folders, along with at least read permission on all files.

Additionally, you must also have share-level READ permissions on the NFS export.


\paragraph{\sphinxstylestrong{Preparing a non-administrator domain user on the NetApp filer}}
\label{\detokenize{mcdmp_app_ug:preparing-a-non-administrator-domain-user-on-the-netapp-filer}}
To configure a NetApp filer from VCC, you can use an account which is not in the administrators group on the NetApp filer, but has some specific privileges.

Perform the following steps on the NetApp filer console to add a non-administrator user, for example, testuser.

\sphinxstylestrong{To create a non-administrator user}
\begin{enumerate}
\item {} 
Create a new role, for example testrole, using the useradmin utility on the filer.

\item {} 
Add the login-* and API-* capabilities to the role.

\end{enumerate}

For example,

\sphinxcode{useradmin role add testrole -a login-*,api-*}.

You can also choose to assign specific capabilities to the role.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Create a new group, for example, testgroup and apply the role testrole to it.

\end{enumerate}

For example,

\sphinxcode{useradmin group add testgroup -r testrole.}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Add the user testdomaintestuser to testgroup.

\end{enumerate}

For example,

\sphinxcode{useradmin domainuser add testdomain\textbackslash{}testuser -g testgroup}


\paragraph{Additional capabilities for adding a non-administrator user account}
\label{\detokenize{mcdmp_app_ug:additional-capabilities-for-adding-a-non-administrator-user-account}}
\sphinxstylestrong{Capability}

\sphinxcode{login-http-admin}

\sphinxstylestrong{Description}

Enables you to log into the NetApp filer and run commands. With this capability, you can get latency statistics (for scan throttling), volume size information, or discover shares.

\sphinxstylestrong{Capability}

\sphinxcode{api-system-get-ontapi-version}

\sphinxcode{api-system-get-version}

\sphinxstylestrong{Description}

Enables you to get the ONTAPI version number and the system version number respectively. These are required to set the login handle context properly. VCC reports a failure when you test the connection to the filer, if these capabilities are absent. Also, if these capabilities are absent, you cannot execute any APIs including those required to discover shares, and get latency statistics.

\sphinxstylestrong{Capability}

\sphinxcode{api-cifs-share-list-iter-start}

\sphinxcode{api-cifs-share-list-iter-next}

\sphinxcode{api-cifs-share-list-iter-end}

\sphinxstylestrong{Description}

Used to discover shares on the NetApp filer. Absence of these capabilities can result in a failure to discover the shares. Optionally, you can add shares manually from the VCC console.

\sphinxstylestrong{Capability}

\sphinxcode{api-perf-object-get-instances-iter-start}

\sphinxcode{api-perf-object-get-instances-iter-next}

\sphinxcode{api-perf-object-get-instances-iter-end}

\sphinxstylestrong{Description}

Used to get CIFS latency information from the NetApp filer, which enables the self-throttling scan feature of Connector. Absence of these APIs can cause scanner to fail if you enable the feature to throttle scanning.

\sphinxstylestrong{Capability}

api-volume-list-info

\sphinxstylestrong{Description}

Used to periodically fetch size information for NetApp volumes.

\sphinxstylestrong{Capability}

\sphinxcode{api-nfs-exportnfs-list-rules}

\sphinxcode{api-nfs-exportnfs-list-rules-2}

\sphinxstylestrong{Description}

Used to discover all NFS shares that are exported from the NetApp filer. If this capability is absent, these NFS shares are not discovered.

\sphinxstylestrong{Capability}

\sphinxcode{api-net-ping}

\sphinxcode{api-net-resolve}

\sphinxstylestrong{Description}

Used to check network connectivity from the filer to the Connector node. These APIs are useful to run some diagnostic checks on the filer. However, such checks can also be done manually by the NetApp administrator, and hence these APIs are not mandatory.


\paragraph{\sphinxstylestrong{Preparing a non-administrator local user on the clustered NetApp filer}}
\label{\detokenize{mcdmp_app_ug:preparing-a-non-administrator-local-user-on-the-clustered-netapp-filer}}
To configure a NetApp cluster file server from VCC, you can use a local user account which is not in the administrators group on the NetApp cluster, but has some specific privileges.

\sphinxstylestrong{To create a non-administrator user}
\begin{enumerate}
\item {} 
Launch a Telnet session with the NetApp Cluster Management host.

\item {} 
Create a new role, for example testrole, using the \sphinxcode{useradmin} utility on the filer.

\item {} 
Run the following commands to create the role with specific privileges:

\end{enumerate}

\sphinxcode{security login role create -role testrole -cmddirname "version" -access all}

\sphinxcode{security login role create -role testrole -cmddirname "vserver cifs" -access readonly}

\sphinxcode{security login role create -role testrole -cmddirname "vserver" -access readonly}

\sphinxcode{security login role create -role testrole -cmddirname "vserver cifs share" -access readonly}

\sphinxcode{security login role create -role testrole -cmddirname "vserver nfs" -access all}

\sphinxcode{security login role create -role testrole -cmddirname "volume" -access readonly}

\sphinxcode{security login role create -role testrole}

\sphinxcode{-cmddirname "network interface show" -access readonly}

The \sphinxcode{network interface} show privilege automatically assigns the following privileges to the role:

\sphinxcode{network interface create}

\sphinxcode{network interface delete}

\sphinxcode{network interface modify}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Run the following command to create a local user, for example, testuser, and assign the role that you created in 3 to the user:

\end{enumerate}

\sphinxcode{security login create -username testuser}

\sphinxcode{-application ontapi -authmethod password -role testrole}


\paragraph{\sphinxstylestrong{Preparing a non-administrator domain user on a NetApp cluster}}
\label{\detokenize{mcdmp_app_ug:preparing-a-non-administrator-domain-user-on-a-netapp-cluster}}
To configure a NetApp cluster from VCC, you can use an account which is not in the administrators group on the NetApp filer, but has some specific privileges. You can use the credentials of a domain user to configure Connector to monitor a NetApp cluster. These credentials are required to discover shares on the NetApp cluster.

Perform the following steps on the NetApp filer console to add a non administrator user, for example, testuser.

\sphinxstylestrong{To use domain user credentials to configure a NetApp cluster}
\begin{enumerate}
\item {} 
Log on using SSH to the NetApp cluster with administrator credentials. Do one of the following:

\end{enumerate}
\begin{itemize}
\item {} 
If the NetApp cluster has a data SVM with a CIFS server that is already created, you can use that data SVM as an authentication tunnel. Use the following command:

\end{itemize}

\sphinxcode{security login domain-tunnel create -vserver name of data SVM}

The following security command displays the specified authentication tunnel:

\sphinxcode{login domain-tunnel show}
\begin{itemize}
\item {} 
If the cluster does not have a data Storage Virtual Machine (SVM) with a CIFS server created, you can use any data SVM in the cluster and join it to a domain by using the \sphinxcode{vserver active-directory create} command.

\end{itemize}

Set the \sphinxcode{-{-}vserver} parameter to the data SVM. Joining a data SVM to a domain does not create a CIFS server or require a CIFS license. However, it enables the authentication of users and groups at the SVM or cluster-level.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Grant a user or a group access to the cluster or SVM with the \sphinxcode{-authmethod} parameter set to domain. Also, create a new role, for example testrole, using the \sphinxcode{useradmin} utility on the filer.

\end{enumerate}
\begin{quote}

The following command enables \textless{}testuser\textgreater{} in the \textless{}DOMAIN1\textgreater{} domain to access the cluster through SSH:

\sphinxcode{cluster1::\textgreater{} security login create -vserver cluster1 \textless{}-user-or-group-name\textgreater{}}
\sphinxcode{\textless{}DOMAIN1\textbackslash{}testuser\textgreater{} -application ontapi -authmethod domain \textendash{}role testrole}
\end{quote}

Where, \sphinxcode{\textless{}cluster1\textgreater{}} is the name of Admin Vserver.

Note the following:
\begin{itemize}
\item {} 
The value of the -user and -group-name parameter must be specified in the format domainnameusername, where \textless{}domain name\textgreater{} is the name of the CIFS domain server and user name is the name of the user or group you want to grant access to.

\item {} 
The user group authentication supports only SSH and ONTAPI for the

\end{itemize}

\sphinxcode{-application} parameter.
\begin{itemize}
\item {} 
If the authentication tunnel is deleted, the directory service logon sessions cannot be authenticated by the cluster, and users and groups cannot access the cluster. The open sessions that were authenticated before the deletion of the authentication tunnel remain unaffected.

\end{itemize}
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
You can also choose to assign specific capabilities to the role.

\end{enumerate}

\sphinxcode{security login role create -role testrole -cmddirname "version" -access all}

Enables you to log into the NetApp filer and run commands. With this capability, you can get latency statistics (for scan throttling), volume size information, or discover shares.

Run the following commands to create the role with specific privileges:

\sphinxcode{security login role create -role testrole -cmddirname "version" -access all}

\sphinxcode{security login role create -role testrole -cmddirname "vserver cifs" -access readonly}

\sphinxcode{security login role create -role testrole -cmddirname "vserver nfs" -access all}

\sphinxcode{security login role create -role testrole -cmddirname "vserver" -access readonly}

\sphinxcode{security login role create -role testrole -cmddirname "volume" -access readonly}

\sphinxcode{security login role create -role testrole \textendash{}cmddirname "statistics" -access readonly}

\sphinxcode{security login role create -role testorle -cmddirname "network interface show" -access readonly}

The network interface show privilege automatically assigns the following privileges to the role:
\begin{itemize}
\item {} 
\sphinxcode{network interface create}

\item {} 
\sphinxcode{network interface modify}

\item {} 
\sphinxcode{network interface modify}

\end{itemize}

You can optionally specify a default role such as admin/vsadmin which already has these privileges.


\paragraph{\sphinxstylestrong{Creating a non-administrator user for an EMC Isilon cluster}}
\label{\detokenize{mcdmp_app_ug:creating-a-non-administrator-user-for-an-emc-isilon-cluster}}
The Connector requires a user account on Isilon to perform automatic discovery of CIFS shares and to list all local groups, group memberships, and local users. The Connector can use a non-administrator account for this purpose. This account can be a local Isilon OneFS account or a domain account.

\sphinxstylestrong{To configure a domain user for discovery and scanning of CIFS shares}
\begin{enumerate}
\item {} 
Log on as an Isilon administrator to the Isilon cluster CLI using SSH or Telnet.

\item {} 
Run the following commands:

\end{enumerate}
\begin{itemize}
\item {} 
To create a role named imrole:

\end{itemize}
\begin{quote}

\sphinxcode{isi auth roles create -{-}name imrole -{-}description Read-only role for Connector}
\end{quote}
\begin{itemize}
\item {} 
To give the user the privileges to log on to the REST API platform framework to get a list of CIFS shares and to list users and groups:

\end{itemize}
\begin{quote}

\sphinxcode{isi auth roles modify imrole -{-}add-user=username@domain}

\sphinxcode{-{-}add-priv-ro=ISI\_PRIV\_SMB -{-}add-priv-ro=ISI\_PRIV\_LOGIN\_PAPI}

\sphinxcode{-{-}add-priv-ro=ISI\_PRIV\_AUTH -{-}add-priv-ro=ISI\_PRIV\_NETWORK}
\end{quote}

\sphinxstylestrong{To configure a local user for discovery of CIFS shares}
\begin{enumerate}
\item {} 
Log on as an Isilon administrator to the Isilon cluster CLI using SSH or Telnet.

\item {} 
Run the following commands:

\end{enumerate}
\begin{itemize}
\item {} 
To create a new local user called imuser

\end{itemize}
\begin{quote}

\sphinxcode{isi auth users create imuser -{-}enabled yes -{-}password xxxxxx}
\end{quote}
\begin{itemize}
\item {} 
To create a role named imrole

\end{itemize}
\begin{quote}

\sphinxcode{isi auth roles create -{-}name imrole -{-}description Read-only role for Connector}
\end{quote}
\begin{itemize}
\item {} 
To grant the user the privileges to log on to the REST API platform framework to get a list of CIFS shares and to list users and groups

\end{itemize}
\begin{quote}

\sphinxcode{isi auth roles modify imrole -{-}add-user=imuser}

\sphinxcode{-{-}add-priv-ro=ISI\_PRIV\_SMB -{-}add-priv-ro=ISI\_PRIV\_LOGIN\_PAPI}

\sphinxcode{-{-}add-priv-ro=ISI\_PRIV\_AUTH -{-}add-priv-ro=ISI\_PRIV\_NETWORK}
\end{quote}


\paragraph{\sphinxstylestrong{Creating a domain user on a Hitachi NAS file server}}
\label{\detokenize{mcdmp_app_ug:creating-a-domain-user-on-a-hitachi-nas-file-server}}
The Connector needs a domain user with administrative privileges on Hitachi NAS EVS to perform the following tasks:
* To discover shares - This credential belongs to a user who has share-level READ permissions on the file server.
* To scan the shares for metadata - This credential must belong to a user with necessary share-level permissions on a Hitachi NAS share.

To be able to scan a Hitachi NAS file server share successfully, you must have the share-level READ permission. Additionally, the folder within the share must have
the following file system ACLs:
* Traverse Folder/Execute File
* List Folder/Read Data
* Read Attributes
* Read Extended Attributes
* Read Permissions

To enable the Connector to successfully scan the shares on a Hitachi NAS file server, ensure that the scanning user has domain level permissions of Allow logon
locally.

\sphinxstylestrong{To create a domain user on the Hitachi NAS file server}
\begin{enumerate}
\item {} 
Log in using SSH to the Hitachi NAS Admin Services EVS using the manager (administrator) credentials.

\item {} 
Execute the following command:

\end{enumerate}

\sphinxcode{localgroup add Administrators \textless{}domain name\textgreater{} /\textless{}username\textgreater{}}


\subsubsection{Adding Native File Server connections}
\label{\detokenize{mcdmp_app_ug:adding-native-file-server-connections}}
An administrator can configure the Native File Server data stores in VCC. You can uses third-party applications, such as 360 Data Analysis to monitor these configured data stores and visualizes the analysis on the Map.

360 Data Analysis supports the following native data store types:
\begin{itemize}
\item {} 
EMC Celerra or EMC VNX

\item {} 
EMC Isilon

\item {} 
Hitachi NAS

\item {} 
NetApp Cluster

\item {} 
NetApp Standalone

\item {} 
Windows File Server (CIFS)

\item {} 
Veritas Files Server

\end{itemize}

Note that the fields are common for most of the Native File Server data stores. The fields that are specific to certain data stores have been exclusively mentioned.

Enter the following details to add a connection for the respective data stores:

\sphinxstylestrong{Fields in Native File Server connections panel}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{2}{\X{1}{2}|}}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a unique name that Connector
Framework uses to identify your Native File Servers data store.
The name can contain up to 260 characters. Note that as the field
name implies, this name is only for display purpose.
\\
\hline
Description
&
Enter a logical description for the content source. The description
can contain up to 1024 characters.
\\
\hline
Server
&
Based on the data store that you want to configure, specify the
server details as appropriate:
\begin{quote}

\sphinxstylestrong{EMC Celerra or VNX:} Enter the fully qualified domain name (FQDN)
address of the CIFS server that is exported by the filer. Do not enter
the IP address of the CIFS server.

\sphinxstylestrong{EMC Isilon:} Enter the FQDN address of the Isilon cluster. It can
be EMC Isilon SmartConnect Cluster name or it can be the DNS resolvable
host name of one of the hosts of the cluster.

\sphinxstylestrong{Hitachi NAS:} Enter the FQDN address of the HNAS file system EVS
that you want 360 Data Analysis to monitor.

\sphinxstylestrong{NetApp Cluster:} Enter the FQDN address of the NetApp Cluster
Management host interface that is used to manage the nodes in the
cluster.

\sphinxstylestrong{NetApp Standalone:} Enter the FQDN address of the filer that
you want 360 Data Analysis to monitor.

\sphinxstylestrong{Windows File Server (CIFS):} Enter the FQDN address of the filer
that you want 360 Data Analysis to monitor. In case of a clustered
Windows File Server, enter the host name or IP address of the cluster.
In case of a clustered file system, make sure to select the This
is a clustered Windows File Server check box.

\sphinxstylestrong{Veritas Files Server:} Enter the FQDN address of the filer that
you want 360 Data Analysis to monitor.

Ensure that the specified FQDN address is resolvable.
\end{quote}
\\
\hline
Port
&
This field is applicable only for EMC Isilon. The port number from the
URL used to access the Isilon OneFS Management Console. This port number
is used by 360 Data Analysis for discovery purposes.
Ensure that this port is not blocked by the Windows firewall.
Default port number is 8080.
\\
\hline
Configuration
&This field is applicable only for EMC Isilon. From the drop-down,
select the configuration that 360 Data Analysis should use to discover
shares on the Isilon cluster.
Select one of the following:

\sphinxstylestrong{Access zones:} Discovers shares from the configured access zones
names, including the shares from the System zone.

\sphinxstylestrong{SmartConnect Zone Name:} 360 Data Analysis uses the SmartConnect
name to discover and scan shares (including shares on system access
zones) whenever SmartConnect is configured.

\sphinxstylestrong{SmartConnect Zone Alias:} SmartConnect aliases are used to ensure
that SmartConnect names work across migration. Thus, aliases take
precedence over SmartConnect names for scanning a share. If the
SmartConnect Zone alias is not available, 360 Data Analysis uses
SmartConnect name to discover shares across access zones. If none of
these are DNS resolvable scan fails for these shares.

Support for using SmartConnect to discover shares is
available for Isilon OneFS version 8.0 and later.
\\
\hline
Scan Schedule
&
Specify the hours when Connector Framework is allowed to perform a
full scan of the configured Native File Server data store.
By default, the scan is allowed all hours of the day. Each full scan
repeats once a day.

To reset the default schedule, click the tiles in the schedule chart.
The scan runs adhering to the active schedules.
\\
\hline
Credentials
&
From the drop-down, select the credential that you want to use
for discovering and scanning the data store.
\\
\hline
Group
&
From the drop-down, select the group that you want to associate
the data store with. Typically, the connections are grouped on the
basis of their location.
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\bigskip\hrule\bigskip



\subsection{Configuring data collection from Microsoft SharePoint On-premises}
\label{\detokenize{mcdmp_app_ug:configuring-data-collection-from-microsoft-sharepoint-on-premises}}

\subsubsection{Setting up the discovery of SharePoint site collections}
\label{\detokenize{mcdmp_app_ug:setting-up-the-discovery-of-sharepoint-site-collections}}\label{\detokenize{mcdmp_app_ug:prereq-sharepoint-onpremises}}
The connector machine uses the Windows PowerShell script to discover on-premises Microsoft SharePoint site collections.
The server script does the following:
\begin{itemize}
\item {} 
Enables Windows PowerShell remoting. PowerShell remoting remains enabled on the SharePoint server machine till it is manually disabled.

\item {} 
Enables CredSSP authentication. CredSSP authentication remains enabled on the SharePoint server machine till it is manually disabled.

\item {} 
Adds the specified user to the SharePoint Shell Admin list which enables the user/group to remotely discover the web applications and site collections on the SharePoint server machine. (The user can be any SharePoint domain administrator or a non-administrator user who has permission on the web application.)

\end{itemize}

Complete the following three steps on the SharePoint Server:


\paragraph{\sphinxstylestrong{Step 1}}
\label{\detokenize{mcdmp_app_ug:step-1}}
\sphinxstylestrong{Enable the PowerShell feature that lets administrators run commands on the remote SharePoint server machine}
\begin{enumerate}
\item {} 
From the connector machine, remotely connect to the SharePoint Server machine.

\item {} 
Run Windows PowerShell as Administrator.

\item {} 
Run the following command to enable the PowerShell execution policy.

\end{enumerate}
\begin{quote}

\sphinxcode{Set-ExecutionPolicy RemoteSigned}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Run the following PowerShell script on the SharePoint Server machine:

\end{enumerate}
\begin{quote}

\sphinxcode{SPServer\_PSRemoting.ps1 -Operation Enable -UserGroupName "domain\textbackslash{}Username"}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
The script, SPServer\_PSRemoting.ps1, is located at the following location on the connector machine:

\end{enumerate}
\begin{quote}

\sphinxcode{\textless{}Install directory\textgreater{}\textbackslash{}InformationMapAgent\textbackslash{}ConnectorFramework\textbackslash{}SharePoint\textbackslash{}PowerShell\textbackslash{}SPOnPremScripts}
\end{quote}

Wait for some time and follow the steps on the console, as described in Step 2.


\paragraph{\sphinxstylestrong{Step 2}}
\label{\detokenize{mcdmp_app_ug:step-2}}
\sphinxstylestrong{Run commands on the SharePoint server machine to add the user account with Read and Execute permissions to permission to the PowerShell Admin list}.

Run the following commands in Windows PowerShell:
\begin{quote}
\begin{description}
\item[{\sphinxcode{Set-PSSessionConfiguration -Name Microsoft.PowerShell32}}] \leavevmode
\sphinxcode{- ShowSecurityDescriptorUI}
\begin{description}
\item[{\sphinxcode{Set-PSSessionConfiguration -Name Microsoft.PowerShell}}] \leavevmode
\sphinxcode{- ShowSecurityDescriptorUI}

\end{description}

\end{description}
\end{quote}

These commands bring up the permissions list window where you must add the user or group account with Read and Execute permissions.


\paragraph{\sphinxstylestrong{Step 3}}
\label{\detokenize{mcdmp_app_ug:step-3}}
\sphinxstylestrong{Configure the SharePoint web application policy for discovery and scan of the site collections.}
\begin{enumerate}
\item {} 
Log in to the SharePoint Central Administration console with Administrator credentials, and click \sphinxstylestrong{Application Management}.

\item {} 
Under the web applications section, click \sphinxstylestrong{Manage Web Applications}.

\item {} 
In the table displaying web application details, select the appropriate web application.

\item {} 
Click \sphinxstylestrong{User Policy}.

\item {} 
In the Policy for web application popup, click \sphinxstylestrong{Add Users}.

\item {} 
Select the appropriate zone. You can select (All Zones) if you want the user to be given permissions on all zones for the web application.

\item {} 
Click \sphinxstylestrong{Next}.

\item {} 
In the \sphinxstylestrong{Choose Permissions} section, select \sphinxstylestrong{Full Control - Has full control}.

\item {} 
Specify whether this account operates as SharePoint System account. If you select the Account operates as System check box, all accesses made by this user account are recorded with the user name, SharePoint System.

\item {} 
Click \sphinxstylestrong{Finish}.

\end{enumerate}

Complete the following steps on the Connector (agent) node:


\paragraph{\sphinxstylestrong{Step 1}}
\label{\detokenize{mcdmp_app_ug:id4}}\begin{enumerate}
\item {} 
Enable PowerShell execution policy on the Connector machine.

\item {} 
On the Connector machine, open Windows PowerShell as administrator and run the following command:

\end{enumerate}
\begin{quote}

\sphinxcode{Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope LocalMachine}
\end{quote}

This command changes the user preference for the Windows PowerShell execution policy to RemoteSigned on the local machine.


\paragraph{\sphinxstylestrong{Step 2}}
\label{\detokenize{mcdmp_app_ug:id5}}
\sphinxstylestrong{Allow delegating fresh credentials with NTLM-only server configuration on the Connector machine.}
\begin{enumerate}
\item {} 
Log in to the Connector machine.

\item {} 
Open \sphinxstylestrong{Local Group Policy Editor}. Click Windows + R, type gedit.msc, and press Enter.

\item {} 
On the Agent server node, navigate to \sphinxstylestrong{Computer configuration \textgreater{} Administrative Templates \textgreater{} System \textgreater{} Credentials Delegation}, and select \sphinxstylestrong{Allow Delegating Fresh Credentials with NTLM-only Server Authentication}.

\item {} 
Select \sphinxstylestrong{Enabled}.

\item {} 
On the resulting window, click \sphinxstylestrong{Enabled} to Allow Delegation with NTLM-only Server Authentication.

\item {} 
Click \sphinxstylestrong{Show} to add the SharePoint Server client domain details to the \sphinxstylestrong{Add Servers} list.

\end{enumerate}

\begin{sphinxadmonition}{note}{Note:}
If local group policies are not enabled on the Connector machine, you must enable them at the domain level.
\end{sphinxadmonition}


\subsubsection{Configuring credentials for Microsoft SharePoint on-premises}
\label{\detokenize{mcdmp_app_ug:configuring-credentials-for-microsoft-sharepoint-on-premises}}
You can use 360 Data Analysis to gain visibility into the unstructured data residing on servers running Microsoft SharePoint. The on-premises data source connector discovers and scans the following SharePoint repositories:
\begin{itemize}
\item {} 
Site collections

\item {} \begin{description}
\item[{Sites and sub-sites}] \leavevmode\begin{itemize}
\item {} 
Document library - Stores documents in the .pdf, .doc, .xls, .txt and other such
file extensions.

\end{itemize}

\end{description}

\end{itemize}

For a given document library, the on-premises connector fetches metadata of files and folders.

Before you add a connection for SharePoint server, you must ensure the following:
\begin{itemize}
\item {} 
The SharePoint server 2016 on premise is installed and running.

\item {} 
SharePoint Connector service is running on a Connector machine. Make sure Connector service port does not conflict with any other service. The default port is 8331.

\item {} 
In case you want to configure visibility into a SharePoint farm, the SharePoint server machine that is added as a connection must be running with the Application Server role.

\item {} 
The Connector machine and the should be in a trusted domain.

\item {} 
The SharePoint Server is geographically close to the Connector machine.

\item {} 
Windows PowerShell is installed on the Connector machine.

\item {} 
PowerShell (PS) remoting (the ability to execute commands on a remote machine) is enabled on the SharePoint server. Discovery credentials are configured on SharePoint server.

\item {} 
For successful discovery and scanning of web application, the credentials must have full control for the target web applications under the web application permission policies.

\end{itemize}

Enter the following details to add a credential for Microsoft SharePoint:
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
Enter a logical name for the credential. The name you
specify here helps you select the relevant credential when
configuring the Microsoft SharePoint connection.
\\
\hline
Description
&
Enter a description for the credential. This field is optional.
\\
\hline
Domain
&
Enter the name of the domain that the Microsoft SharePoint server
is a part of.
\\
\hline
User name
&
Enter the account ID for the Exchange account.
\\
\hline
Password
&
Enter the password for the Exchange account.
\\
\hline
Confirm Password
&
Re-enter the password for verification.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}


\subsubsection{Adding a Microsoft SharePoint on-premises connection}
\label{\detokenize{mcdmp_app_ug:adding-a-microsoft-sharepoint-on-premises-connection}}
\sphinxstylestrong{Adding a connection for Microsoft SharePoint on-premises}
\begin{quote}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Connector Framework uses
to identify your SharePoint web application. The name that you enter
in this field represents a content source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
SharePoint web application.
\\
\hline
Web Application URL
&
Enter the URL of the SharePoint web application.
\\
\hline
Application Server
&
Enter the FQDN of the SharePoint server.
\\
\hline
Credential
&
From the drop-down, select the appropriate saved credential for the
SharePoint Server you want to access.
\\
\hline
Group
&
Select the group of the connector node which is responsible for
scanning the SharePoint server is selected by default.
\\
\hline
Scan Schedule
&
Specify the hours when Connector Framework is allowed to perform a
full scan of the configured SharePoint server. By default, the scan is
allowed all hours of the day. Each full scan repeats once a day.
To reset the default schedule, click the tiles in the schedule chart.
The scan runs adhering to the active schedules.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\end{quote}

\begin{sphinxadmonition}{note}{Note:}
If the connector encounters a double-byte or UTF-8 character, the scan of the SharePoint server fails. In such as case, edit the connector\_context.property file located in the Install folder. Change the property to SP\_Discovery\_output\_type=file, and restart the connector service.
\end{sphinxadmonition}


\bigskip\hrule\bigskip



\subsection{Managing a NetBackup connection}
\label{\detokenize{mcdmp_app_ug:managing-a-netbackup-connection}}
In your data centers, the NetBackup software or appliances protect the files and folders on your file servers. The metadata pertaining to these files and folders is stored in the catalog of your NetBackup master server(s). Using NetBackup as the source of metadata has the benefit of not requiring additional scans of your file servers and allows Loom to collect data efficiently, with minimum impact on your front-end application. It also provides an efficient way of gathering information from an organization’s primary content sources.

The Loom platform facilitates the collection of metadata from NetBackup catalogs. NetBackup stores data in terms of policies and their corresponding backup images. This data is stored under one master catalog. A policy defines the backup selection and schedule information. On every run it will generate a new backup image.
The backup image contains the actual metadata. It contains all the files which are being backed up in one policy run plus the contents for that run.
When you add a NetBackup connection in the Connection Center, the Loom Data Engine stores the policy name and expanded backup selection information.

During discovery, the policy backup selection is expanded and stored as content sources on the Data Engine. The Loom Data Plane periodically fetches the metadata from the most recent full backup and subsequent incremental and full backups from the content sources. It then securely uploads a subset of the collected metadata to the Loom platform which is hosted in the Veritas cloud.

The Data Plane fetches this information for the following type of NetBackup policies:
\begin{itemize}
\item {} 
Standard

\item {} 
MS-Windows

\item {} 
NDMP

\item {} 
VMware

\item {} 
Hyper-V

\end{itemize}

For more information about NetBackup and NetBackup policies, see the Veritas NetBackup documentation.

The Loom Alpha 2 release supports collection of metadata for the following NetBackup versions:

Veritas NetBackup, 7.6.x, 7.7.x, 8.0, 8.1.

Veritas NetBackup Appliance, 2.6 or later.


\subsubsection{Prerequisites for adding a NetBackup connections}
\label{\detokenize{mcdmp_app_ug:prerequisites-for-adding-a-netbackup-connections}}
Before you can add a NetBackup connection, the following prerequisites steps must be complete:
\begin{itemize}
\item {} 
Ensure that the on-premises Data Engine is deployed for the tenant.

\item {} 
Configure Data Engine’s access to the master server.

\end{itemize}


\subsubsection{Configure access to a NetBackup master server}
\label{\detokenize{mcdmp_app_ug:configure-access-to-a-netbackup-master-server}}
The Loom platform pulls metadata from the NetBackup master server. To enable Loom to access the master server, you must add the IP address of the Data Plane to the master server. The network name for the Data Engine server must be resolvable in DNS for reverse name lookup (for example, loom.company.com resolving to 192.168.0.150).

Note that you must complete the procedure only once for every NetBackup master server that you want the Agent to connect with.

To configure communication between Loom and the master server
\begin{enumerate}
\item {} 
On the NetBackup Administration Console, click \sphinxstylestrong{NetBackup Management} \textgreater{} \sphinxstylestrong{Host Properties} \textgreater{} \sphinxstylestrong{Master Servers}.

\item {} 
On the \sphinxstylestrong{Master Servers Properties} window, click \sphinxstylestrong{Servers}.

\item {} 
On the \sphinxstylestrong{Servers} pop-up, click \sphinxstylestrong{Additional Servers}.

\item {} 
Click \sphinxstylestrong{Add}, and enter the IP address of the master and worker nodes on which the Data Engine is deployed.

\end{enumerate}

This step adds the Data Engine servers to the list of servers that can access the NetBackup master server.


\subsubsection{Adding a NetBackup connection}
\label{\detokenize{mcdmp_app_ug:adding-a-netbackup-connection}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Field
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Description
\unskip}\relax \\
\hline
Display Name
&
This is a free-form field. Enter a name that Loom uses
to identify your NetBackup connection. The name that you enter
in this field represents a content source in 360 Data Analysis.
\\
\hline
Description
&
Enter a logical description to help you to uniquely identify the
NetBackup connection.
\\
\hline
Server
&
Enter the FQDN of the NetBackup master server.
\\
\hline
Credential
&
From the drop-down, select the appropriate saved credential for the
NetBackup master server that you want to access.
\\
\hline
Group
&
Select the group of the Data Engine that is responsible for
scanning the NetBackup server.
\\
\hline
Scan Schedule
&
Specify the hours when Loom is allowed to perform a
full scan of the configured NetBackup master server. By default, the
scan is allowed all hours of the day. Each full scan repeats once a day.
To reset the default schedule, click the tiles in the schedule chart.
The scan runs adhering to the active schedules.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\bigskip\hrule\bigskip


This document is work in progress.



\renewcommand{\indexname}{Index}
\printindex
\end{document}